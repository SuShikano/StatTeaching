---
title: "Using the data.generation function"
author: "Susumu Shikano"
date: "Last compiled at `r format(Sys.Date(), '%d. %B %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading the function from github.

We first load the function, which is used to generate multiple datasets under specified assumptions.

```{r}
source("https://raw.githubusercontent.com/SuShikano/StatTeaching/main/func/data_generation.R")


```

## Generating two different datasets

Let's generate 2 different datasets (n.sim=2) with the sample size of 100 (sample.size=500) and with the true regression model with 
only one independent variable (n.iv=1) whose mean is 3 (x.mu=3) and variance is 1 (x.Sigma=as.matrix(1,nrow=1)). The true regression model has the parameter value 1 for the intercept and 5 for the slope (para=c(1,5)). Its error has the uniform distribution (err.dist = "uniform"") with the range of 20 centered on 0 (err.disp = 20).

```{r}

samples01 <- data.generation(sample.size=100,
                             n.sim=2,
                             n.iv=1,
                             x.mu=3,
                             x.Sigma=as.matrix(1,nrow=1),
                             para=c(1,5),
                             err.dist = "uniform",
                             err.disp = 20)
```

## Describing the generated data

The first generated dataset looks as follows:

```{r}
head(samples01$generated.data[[1]])
```


... and the second generated data:

```{r}
head(samples01$generated.data[[2]])
```

You can also plot y and x1 of both datasets:

```{r}
par(mfrow=c(1,2))

for (i.fig in 1:2){
  plot(samples01$generated.data[[i.fig]]$y ~ samples01$generated.data[[i.fig]]$X1,
       xlab="X1",ylab="y",
       main=paste("generated data",i.fig)
       )
}
```


## Regressing y on X1.

By using each of the datasets, we can regress y on X1. As one can see below, both results are not identical with the true model $\hat y = 1 + 5 \times X1$.

```{r}

lm.outs <- vector(mode="list",length=2)
for (i in 1:2){
  print(paste("Generated data",i))
  
  lm.outs[[i]] <- lm(y ~ X1, data=samples01$generated.data[[i]])
  
  print(
    summary(lm.outs[[i]])
    )
}

```

The estimated regression lines go through the generated datasets:

```{r}
par(mfrow=c(1,2))

for (i.fig in 1:2){
  plot(samples01$generated.data[[i.fig]]$y ~ samples01$generated.data[[i.fig]]$X1,
       xlab="X1",ylab="y",
       main=paste("generated data",i.fig)
       )
  abline(reg=lm.outs[[i.fig]])
}
```



## Checking the performance of the regression models using 1000 datasets

We generate 1000 datasets (n.sim=1000) based on the same set-up above.

```{r}

samples02 <- data.generation(sample.size=100,
                                  n.sim=1000,
                                  n.iv=1,
                             x.mu=3,
                             x.Sigma=as.matrix(1,nrow=1),
                             para=c(1,5),
                             err.dist = "uniform",
                             err.disp = 20)


```

Using each of the generated datasets, we regress y on X1 and collect the results:


```{r}

all.intercepts <- all.slopes <- matrix(NA,nrow=1000,ncol=4)
for(i.data in 1:1000){
  this.result <-     
     summary(lm(y ~ X1, 
                data=samples02$generated.data[[i.data]]))$coefficients
  all.intercepts[i.data,] <- this.result[1,]
  all.slopes[i.data,] <- this.result[2,]
} 


```

Let's check the distribution of the point estimates.

```{r}

par(mfrow=c(1,2))
for (i.fig in 1:2){
  if (i.fig==1) to.plot <- all.intercepts[,1]
  if (i.fig==2) to.plot <- all.slopes[,1]
  
  hist(to.plot,
       main=c("intercept","slope")[i.fig],
       xlab="Estimates",
       freq=F)
  legend("topright",
         paste("mean",round(mean(to.plot),2)),
         bty="n")
  
}


```

The means of the estimates are very close to the true value. If you use infinitely large number of datasets, the means are identical with the true value (no bias).

The regression lines based on 1000 datasets can be visualized in the following figure:

```{r}

plot(0,xlim=c(0,6),ylim=c(-10,40),type="n",
     xlab="X1",ylab="y")
# plot the estimated regression lines (grey color)
for(i.data in 1:1000){
  abline(coef=c(all.intercepts[i.data,1] ,all.slopes[i.data,1] ),
         col="grey")
} 
# plot the true regression line (black)
abline(coef=c(1,5)) 
legend("topleft",
       c("estimated line","the true line"),
       lty=1,
       col=c("grey","black"),
       bty="n")



```

The black line is the true regression line, around which the grey estimated regression lines based on the random samples can be found.

Let's check the 95% confidence intervals of the slope estimates:

```{r fig.height = 6, fig.width = 4, fig.align = "center"}
ci.prob <- .95
lo.prob <- (1-ci.prob)/2
up.prob <- 1-lo.prob

lo.bounds <- all.slopes[,1] + qt(lo.prob,df=100-2)*all.slopes[,2]
up.bounds <- all.slopes[,1] + qt(up.prob,df=100-2)*all.slopes[,2]

xrange <- range(c(lo.bounds,up.bounds))

without.true.value <- ifelse(lo.bounds>5 | up.bounds<5,1,0)

plot(all.slopes[,1],1:1000,xlim=xrange,
     pch=19,
     col=c("black","red")[without.true.value+1],
     xlab="slope estimates",
     ylab="datasets")
for (i.data in 1:1000){
  lines(c(lo.bounds[i.data],up.bounds[i.data]),rep(i.data,2),
        col=c("black","red")[without.true.value[i.data]+1])
}
abline(v=5,col="white",lwd=2)


```

`r mean(without.true.value)*100`% of all confidence intervalls (red color) do not include the true slope. 


