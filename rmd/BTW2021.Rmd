---
title: 'Example regression analysis (Data: Official Results of the
  2021 German Federal Election)'
author: "Susumu Shikano"
date: "Last compiled at `r format(Sys.Date(), '%d. %B %Y')`"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# loading some functions
source("https://raw.githubusercontent.com/SuShikano/StatTeaching/main/func/ci_sample_mean.R")

```

## Preparing data

We download the official results of 2021 German Federal Election. 

```{r, echo = TRUE}

library(foreign)

url.result <- "https://www.bundeswahlleiter.de/dam/jcr/fc2afe29-c086-43eb-bc34-a48356dee154/btw21_kerg.csv"
# reading the results
rawdata <- read.csv(url.result,
                    skip=5,sep=";",header = FALSE,encoding="UTF-8")

# reading the labels
main.labels <- read.csv(url.result,
                        skip=2,nrows=1,sep=";",header = FALSE,
                        colClasses = "character",encoding="UTF-8")

# reading the second line of the labels
first.second.votes <- read.csv(url.result,
                               skip=3,nrows=1,sep=";",header = FALSE)

first.second.votes[1:3] <- "aaa" 

# delete previous results and state-level results
result21 <- rawdata[,!is.na(first.second.votes)]
result21 <- result21[result21[,3]!=99 & !is.na(result21[,3]),]

# make the labels
main.labels <- main.labels[!is.na(first.second.votes)]
main.labels[seq(5,length(main.labels),by=2)] <-    main.labels[seq(4,length(main.labels)-1,by=2)] # adding the label for SMD and PR votes

main.labels <- paste(main.labels,first.second.votes[!is.na(first.second.votes)])
main.labels <- gsub("aaa","",main.labels)

main.labels <- gsub("ä","ae",main.labels)
main.labels <- gsub("Ä","Ae",main.labels)
main.labels <- gsub("ö","oe",main.labels)
main.labels <- gsub("Ö","Oe",main.labels)
main.labels <- gsub("ü","ue",main.labels)
main.labels <- gsub("Ü","Ue",main.labels)

colnames(result21) <- main.labels

```

After data cleaning, the dataset contains 299 electoral districts as unit.

We further download socio-demographic data for the districts.

```{r, echo = TRUE}

url.sociodem <- "https://www.bundeswahlleiter.de/dam/jcr/b1d3fc4f-17eb-455f-a01c-a0bf32135c5d/btw21_strukturdaten.csv"
rawdata <- read.csv(url.sociodem,
                    skip=8,sep=";",dec = ",",
                    header = TRUE,encoding="UTF-8")

# delete the state-level results
sociodem <- rawdata[rawdata[,2]<500,]

```

## Obtaining variables as vectors

- old.gen: percentage of the older generation (age 60+)
- gdp: GDP per capita
- eligible: number of eligible citizens to vote
- valid: number of valid votes
- tunrout: turnout rate (in %)
- union.pr: vote share of CDU/CSU (in %)

```{r, echo = TRUE}

gdp <- sociodem[,grep("Bruttoinlandsprodukt",names(sociodem))]

old.gen <- sociodem[,grep("Alter",names(sociodem))]
old.gen <- old.gen[,5] + old.gen[,6]

unemployment <- sociodem[,grep("Arbeitslosenquote",names(sociodem))]
unemp.total <- unemployment[,grep("insgesamt",names(unemployment))]
unemp.young <- unemployment[,grep("15.bis.24.Jahre",names(unemployment))]

eligible <- result21[,grep("Wahlberechtigte Zweitstimmen",names(result21))]
valid <- result21[,grep("Gueltige Stimmen Zweitstimmen",names(result21))]

turnout <- valid *100/ eligible

cdu.pr <- result21[,grep("Christlich Demokratische Union Deutschlands Zweitstimmen",names(result21))]

csu.pr <- result21[,grep("Christlich-Soziale Union in Bayern e.V. Zweitstimmen",names(result21))]

union.pr <- cdu.pr
union.pr[is.na(union.pr)] <- csu.pr[is.na(union.pr)]

union.pr <- union.pr*100/valid


afd.pr <- result21[,grep("Alternative fuer Deutschland Zweitstimmen",names(result21))]
afd.pr <- afd.pr *100/valid


```

## Joint distributions

We observe two joint distributions:

```{r, echo = TRUE}

par(mfrow=c(1,2))
plot(gdp,turnout,xlab="GDP per capita",ylab="Turnout (%)")
plot(old.gen,union.pr,xlab="Age 60+ (%)",ylab="CDU/CSU (%)")

```


## Variances and covariances

```{r, echo = TRUE}
cov(cbind(gdp,old.gen,turnout,union.pr))
```

The diagonal elements are variances. The off-diagonal elements are covariances. Here, you have to be able to calculate the slope of the regression lines. Calculate the regression slopes (Turnout on GDP and CDU/CSU on older generation).

## Calculating the regression slopes

To obtain the regression slope, we can divide covariance of the dependent and independent variable by the variance of independent variable. We now estimate the slop for the following regression model:

- turnout = $\beta_0$ + $\beta_1$ gdp + u
```{r, echo = TRUE}
cov(turnout,gdp)/var(gdp)
```

- union.pr = $\beta_0$ + $\beta_1$ old.gen + u
```{r, echo = TRUE}
cov(union.pr,old.gen)/var(old.gen)
```

## Simple regression models (Turnout on GDP)

```{r, echo = TRUE}
lm.out.1 <- lm(turnout ~ gdp)
summary(lm.out.1)
```
You can check whether your hand-calculated regression slope corresponds to the output.


## Simple regression models (Vote for CDU/CSU on age 60+)

```{r, echo = TRUE}
lm.out.2 <- lm(union.pr ~ old.gen)
summary(lm.out.2)
```

## Simple regression models (graphical presentation)

```{r, echo = TRUE}
par(mfrow=c(1,2))
plot(gdp,turnout,xlab="GDP per capita",ylab="Turnout (%)")
abline(lm.out.1)
plot(old.gen,union.pr,xlab="Age 60+ (%)",ylab="CDU/CSU (%)")
abline(lm.out.2)
```


## Regression parameters as random parameters

Now, we change the perspective and treat our data as population, from which we draw random samples. More concretely, we randomly sample 100 districts from 299 all districts and draw the regression line. This is repeated for 1000 times.

```{r, echo = TRUE}
n.iter <- 1000
sample.size <- 100

estimates <- matrix(NA,ncol=2,nrow=n.iter)
plot(old.gen,union.pr,xlab="Age 60+ (%)",ylab="CDU/CSU (%)")
for (i in 1:n.iter){
  this.sample <- sample(1:length(union.pr),size = sample.size)
  this.lm.out <- lm(union.pr[this.sample] ~ old.gen[this.sample])
  estimates[i,] <- coefficients(this.lm.out)
  abline(this.lm.out,col="grey")
}
```

We can observe the distribution of slopes based on 1000 samples.

```{r, echo = TRUE}
hist(estimates[,2],xlab="Slope",main="")
```

The mean of this distribution is `r round(mean(estimates[,2]),4)`.

This estimator (estimand: the regression line based on all 299 districts) is likely to be biased due to the violation of the zero conditional mean assumption.


To see this, we can check the joint distribution of the errors and the independent variable (Age 60+).

```{r, echo = TRUE}
par(mfrow=c(1,2))
plot(old.gen,union.pr,xlab="Age 60+ (%)",ylab="CDU/CSU (%)")
abline(lm.out.2)

errors <- lm.out.2$residuals

plot(old.gen,errors,xlab="Age 60+ (%)",ylab="Errors")
abline(h=0)


```

If we calculate the mean errors for different x values, it is clearly to see that the estimated regression line tends to over-predict the CDU/CSU share for the districts with about 25% and those with 35% or more.  

```{r}
y.range <- range(errors)
x.range <- range(old.gen)

plot(errors ~ old.gen,ylab="Errors",xlab="Age 60+",
     xlim=x.range,ylim=y.range)

x.values <- seq(min(old.gen),max(old.gen),length=25)
x.interval <- x.values[2] -x.values[1]

conditional.mean <- lower.b <- upper.b <- rep(NA,length(x.values))
for (i in 1:length(conditional.mean)){
  
  selected.error <- errors[(old.gen>(x.values[i]-x.interval)) & 
                                             (old.gen<(x.values[i]+x.interval)) ]
  conditional.mean[i] <- mean(selected.error)
  this.ci <- ci.sample.mean(selected.error)
  lower.b[i] <- this.ci$lower.b
  upper.b[i] <- this.ci$upper.b
  
}

par(new=T)
plot(conditional.mean ~ x.values,ann=F,axes=F,
     xlim=x.range,ylim=y.range,
     col="red",pch=19,type="b")
abline(h=0,lty=2,lwd=3)
for (i in 1:length(conditional.mean)){
  lines(rep(x.values[i],2),c(upper.b[i],lower.b[i]),col="red",lwd=2)
}


```


Furthermore, the homoscedasticity assumption seems to be violated, as well. The red points in the below figure show the standard deviations of the errors for different x values with their 95%-confidence intervals (vertical red lines). Accordingly, the error variance is larger for smaller x values.


```{r}
y.range <- range(errors)
x.range <- range(old.gen)

plot(errors ~ old.gen,ylab="Errors",xlab="Age 60+",
     xlim=x.range,ylim=y.range)

x.values <- seq(min(old.gen),max(old.gen),length=25)
x.interval <- x.values[2] -x.values[1]

conditional.sd <- lower.b <- upper.b <- rep(NA,length(x.values))
for (i in 1:length(conditional.sd)){
  
  selected.error <- errors[(old.gen>(x.values[i]-x.interval)) & 
                                             (old.gen<(x.values[i]+x.interval)) ]
  conditional.sd[i] <- sd(selected.error)
  
  dof <- length(selected.error)-1
  
  lower.b[i] <- sqrt(conditional.sd[i]^2*dof/qchisq(0.975,df=dof))
  upper.b[i] <- sqrt(conditional.sd[i]^2*dof/qchisq(0.025,df=dof))
  
}

par(new=T)
plot(conditional.sd ~ x.values,ann=F,axes=F,
     xlim=x.range,ylim=y.range,
     col="red",pch=19,type="b")
abline(h=0,lty=2,lwd=3)
for (i in 1:length(conditional.sd)){
  lines(rep(x.values[i],2),c(upper.b[i],lower.b[i]),col="red",lwd=2)
}


```

## Variance decomposition and Goodness of Fit

To discuss the variance decomposition and goodness of fit, we come back to the first regression model:

```{r}
summary(lm.out.1)

plot(gdp,turnout,xlab="GDP per capita",ylab="Turnout (%)")
abline(lm.out.1)

```

Based on the regression result above, we can calculate the following three sum of squares

```{r}

SST <- mean((turnout - mean(turnout))^2)
SST

SSE <- mean((lm.out.1$fitted.values - mean(turnout))^2)
SSE

SSR <- mean((lm.out.1$residuals )^2)
SSR

r.squared <- SSE/SST
r.squared
```

Obviously, the total sum of squares of y can be decomposed to SSE and SSR. And the share of SSE in SST is the r-squared, which is a goodness of fit measure of the model.

## Estimating variances of errors and OLS estimates

```{r, echo = TRUE}
SSR <- sum(lm.out.1$residuals^2) # residual square sum
sigma2.hat <- SSR/lm.out.1$df.residual # estimated variance of errors

lm.out.1$df.residual
sqrt(sigma2.hat)

SST.x <- sum((gdp-mean(gdp))^2) # total square sum of X (GDP)

sigma2.beta1 <- sigma2.hat/SST.x 
sqrt(sigma2.beta1) # standard error of beta_1
```

Check this result with the output above. 


## Multiple regression analysis



```{r}
pairs(cbind(afd.pr,gdp,unemp.total,unemp.young),
      labels=c("AfD (%)","GDP per capita","Unemployment (%)","Unemp.young (%)"),
      lower.panel = NULL)

```


```{r, echo = TRUE}
srm.out <- lm(afd.pr ~ gdp)
summary(srm.out)

plot(gdp,afd.pr,xlab="GDP per capita",ylab="AfD (%)")
abline(srm.out)
```

The slope estimate has a very small effect size (`r round(coef(srm.out)[2],6)`), which is due to the large number of the independent variable (GDP per capita). That is, one Euro increase in GDP leads to only a small change in the AfD vote share. 

We can now change the scale of the independent variable to thousand Euro and estimate the same model.

```{r}
gdp.tsd <- gdp/1000

srm.out <- lm(afd.pr ~ gdp.tsd)
summary(srm.out)

plot(gdp.tsd,afd.pr,xlab="GDP per capita (TSD)",ylab="AfD (%)")
abline(srm.out)

```

Now the slope estimate is much larger since it is about the change of AfD vote share for 1000 Euro increase in GDP.


If we add another variable (unemployment of younger citizens), the point estimate of the GDP becomes closer to zero.

```{r, echo = TRUE}
mrm.out <- lm(afd.pr ~ gdp.tsd + unemp.young)
summary(mrm.out)
```

To see why this differences in the estimates between two models, we can regress gdp on unemployment.

```{r, echo = TRUE}
gdp.unemp <- lm(gdp.tsd ~ unemp.young)
summary(gdp.unemp)
```
GDP is negatively correlated with the youth unemployment rate. We can now regress the share of AfD on the residual (part of GDP which cannot be predicted by the youth unemployment rate.)

```{r, echo = TRUE}
residuals <- gdp.unemp$residuals
afd.resid <- lm(afd.pr ~ residuals)
summary(afd.resid)
```

Compare with the result of the multiple regression model. The point estimate is identical with that of GDP.


## The omitted variable bias

```{r, echo = TRUE}
simple <- lm(afd.pr ~ gdp.tsd)
summary(simple)
multiple <- lm(afd.pr ~ gdp.tsd + unemp.young)
summary(multiple)
```

If the model with two independent variables is true, the first one is misspecified. The bias can be calculated as follows:

```{r, echo = TRUE}
# extracting the relevant estimates
beta.1.s <- simple$coefficients[2]
beta.1.m <- multiple$coefficients[2]
beta.2.m <- multiple$coefficients[3]

# regress the omitted variable on the other variable
unemp.gdp <- lm(unemp.young ~ gdp)

# extracting the coefficient
delta.1 <- unemp.gdp$coefficients[2]

# biased estimate in SRM
beta.1.s
# true estimate in MRM
beta.1.m
# reconstructing the biased estimate by using the MRM estimates
beta.1.m + beta.2.m*delta.1
# bias
beta.2.m*delta.1

```

The bias constitutes the effect of the omitted variable and its relationship with the other variable.

## Homoskedasticity

The one of the Gauss-Markov-assumptions is about the variance of errors. While we cannot directly observe the errors, we can obtain certain information about the errors by observing residuals and comparing them with independent variables/fitted values.

```{r, echo = TRUE}
# extracting the relevant estimates
lm.out <- lm(afd.pr ~ gdp + unemp.young)
summary(lm.out)

par(mfrow=c(1,3))
plot(gdp,lm.out$residuals,
     xlab="Predicted value",
     ylab="GDP per capita")
abline(h=0,lty=2)
plot(unemp.young,lm.out$residuals,
     xlab="Predicted value",
     ylab="Unemployment")
abline(h=0,lty=2)
plot(lm.out$fitted.values,lm.out$residuals,
     xlab="Predicted value",
     ylab="Residuals")
abline(h=0,lty=2)



```

The distributions indicate violence of the homoskedasticity assumption, thus the biased estimates of the regression coefficients' variances.


