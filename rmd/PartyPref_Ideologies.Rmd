---
title: "Example regression analysis: Party preference and ideological orientation"
author: "Susumu Shikano"
date: "Last compiled at `r format(Sys.Date(), '%d. %B %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

source("https://raw.githubusercontent.com/SuShikano/StatTeaching/main/func/naive_var.R")
source("https://raw.githubusercontent.com/SuShikano/StatTeaching/main/func/ci_sample_mean.R")

```

## Substantive motivation

You are interested in what factor determines citizens' evaluation of political parties. More specifically, you are interested in citizens' like/dislike of parties. You are speculating ideological proximity/distance behind this kind of evaluation. That is, a citizen likes a party whose ideological position is closer to their positions than another party with a more distant position. 

To test your idea, you conducted online surveys with students visiting a lecture. To obtain the most important variables, that is, respondents' like/dislike and proximity to different parties, you asked the following questions:


```{r , echo=FALSE}
knitr::include_graphics("image/Question_Skalo.png")

knitr::include_graphics("image/Question_LR.png")

```


We can load the dataset and check its content:



```{r }
# Loading the data file
load("data/Lec_Statistics_Ideology_Data.RData")

# Check which object is loaded
ls()

# Check the inside of the data
head(long.data,n=10)


```

The data contains the following variables.

- id: Identification number of responses
- wave: survey wave
* party: Which party is at stake?
    + 1: CDU; 2: CSU; 3: SPD; 4: Greens; 5: FDP; 6: AfD; 7: Linke
- skalo: Respondents' like/dislike of a party (+5 like/-5 dislike)
- lr.self: Respondents' self placement on the left-right ideology scale (+5 right most/-5 left most)
- lr: Respondents' placement of the party on the left-right ideology scale (+5 right most/-5 left most)
- time: Response time for the whole skalometer question page (in seconds)

In the dataset, we have `r nrow(long.data)` observations. Note that each observation corresponds to a response. Since a respondent can give multiple responses for different parties, s/he can appear many times in this dataset. 


## Univariate discriptive statistics

We can start with the respondents' like/dislike of parties. Below its distribution:

```{r }
plot(table(long.data$skalo),type="h",main="Like/Dislike",ylab="Frequency")

```

Assume that this measurement contains random errors which follow a normal distribution $N(0,0.09)$:

```{r}
error.dist.density <- function(x){
  dnorm(x,mean=0,sd=sqrt(0.09))
}
plot(error.dist.density,xlim=c(-2,2),
     main="Normal distribution: N(0,0.09)",
     ylab="Density",
     xlab="Measurement Error")
```

For each of 2504 observations, we generate random errors and compute the true value behind the measurement:

```{r}
set.seed(1234567) # Setting a seed so that the "random" errors can be replicated.
measurement.error <- rnorm(nrow(long.data),mean=0,sd=sqrt(0.09))

long.data$skalo.true <- long.data$skalo - measurement.error

par(mfrow=c(1,2))

plot(table(measurement.error),type="h",xlab="measurement error",ylab="Frequency")
plot(table(long.data$skalo.true),type="h",xlab="true value",ylab="Frequency")
```

This way of plot is not informative since both variables can take all real numbers and each value appears only once. That is, the measurement errors and true values here are continuous random variables. Instead of counting all possible values, we can group the values for certain ranges and plot their densities in  histograms.

```{r}

par(mfrow=c(1,2))

hist(measurement.error,xlab="measurement error",ylab="Density",freq=FALSE,
     main="Measurement error")
hist(long.data$skalo.true,xlab="true value",ylab="Density",freq=FALSE,
     main="True values")

```

We can compute some descriptive statistics of the true value. More specifically, we can obtain mean, variance and standard deviation.

```{r}
print("The number of valid observations")
print(valid.n.skalo <- length(long.data$skalo.true))

print("The mean response")
print(mean.skalo <- sum(long.data$skalo.true)/valid.n.skalo)

print("Variance of the responses")
print(var.skalo <- sum((long.data$skalo.true - mean.skalo)^2)/valid.n.skalo )

print("Standard deviation of the responses")
print(sd.skalo <- sqrt(var.skalo))

```

The above procedure is a bit tedious since you are programming hand-rolled. Fortunately, there are ready-made built-in functions in R:

```{r}
length(long.data$skalo.true)
mean(long.data$skalo.true)
var(long.data$skalo.true)
sd(long.data$skalo.true)
```

If you compare the results with those above, however, there are slight differences in variance and standard deviation. This is because `var` and `sd` functions compute the unbiased estimates. That is, you divide not by the number of observations (`r valid.n.skalo`), but $n-1$ (`r valid.n.skalo-1`) to obtain the variance.

To obtain just a variance (not an estimate), you should correct as follows:

```{r}
var(long.data$skalo.true)*(valid.n.skalo-1)/valid.n.skalo

```

In R, one can also define your own function for the naive variance:


```{r}
naive.var <- function(x){
  diff.to.mean <- x - mean(x)
  squared.diff <- diff.to.mean^2
  output <- mean(squared.diff)
  output
}

naive.var(long.data$skalo.true)
```

Below, we can also observe further variables.


```{r }
hist(long.data$lr.self,br=seq(-5.5,5.5,by=1),main="",
     xlab="Left-Right Self-Placement",
     freq=FALSE)

mean(long.data$lr.self)
naive.var(long.data$lr.self)
```




```{r }
hist(long.data$lr,br=seq(-5.5,5.5,by=1),
     xlab="Left-Right Placement of parties",main="",
     freq=FALSE)

mean(long.data$lr)
naive.var(long.data$lr)

```
## Bivariate relationship between the party placement and self-placement

Now we can inspect the joint distribution of the the left-right party-placement and self-placement. We can just plot the distribution:


```{r}
plot(long.data$lr ~ long.data$lr.self,
     xlab="Self-placement",ylab="Party placement")
```

This is however not so informative since both variables are discrete and many responses have the same value combination. Therefore, we cannot see here how many responses are behind different value combinations. Here, we can just add small random values to both measures and plot them again:  

```{r}
plot(jitter(long.data$lr) ~ jitter(long.data$lr.self),
     xlab="Self-placement",ylab="Party placement")
```

The relationship of both variables is not clear to see. We can now calculate two summary statistics: covariance and correlation:

```{r}
cov(long.data$lr , long.data$lr.self)
cor(long.data$lr , long.data$lr.self)
```

Here again, we have to be careful that `cov` gives the unbiased estimate. For simple covariance, you need to write your own function:

```{r}

naive.cov <- function(x,y){
  mean((x - mean(x))*(y - mean(y)))
}

naive.cov(long.data$lr , long.data$lr.self)

```



## Creating a new variable

We can now create new variables based on the existing variables:

* Difference between the party placement and self-placement
* Absolute value of the above difference
* Squared value of the above difference

```{r }

long.data$lr.dist.dif <- long.data$lr - long.data$lr.self
long.data$lr.dist.abs <- abs(long.data$lr.dist.dif)
long.data$lr.dist.sqr <- long.data$lr.dist.dif^2

```


```{r}
mean(long.data$lr.dist.dif)
naive.var(long.data$lr.dist.dif)
```
These values can be also obtained from the statistics of the variables, based on which the variable was created:


```{r}
mean(long.data$lr) - mean(long.data$lr.self)
naive.var(long.data$lr) + naive.var(long.data$lr.self) - 2*naive.cov(long.data$lr,long.data$lr.self)

```


## Bivariate relationship between interested variables

Now we can inspect the joint distribution of the like/dislike variable and the absolute distances. We can just again plot the distribution with small random numbers:


```{r }
plot(long.data$skalo.true ~ long.data$lr.dist.abs,
     xlab="Ideological Distance (abs)",
     ylab="Like/Dislike (true value)")

naive.cov(long.data$skalo.true, long.data$lr.dist.abs)

cor(long.data$skalo.true, long.data$lr.dist.abs)
```

Now, the same exercise for the squared distances:

```{r }
plot(long.data$skalo.true ~ long.data$lr.dist.sqr,
     xlab="Ideological Distance (sqr)",
     ylab="Like/Dislike (true value)")

naive.cov(long.data$skalo, long.data$lr.dist.sqr)

cor(long.data$skalo, long.data$lr.dist.sqr)
```


## Central limit theorem

In this section, we learn the central limit theorem by using the generated true response to party evaluation. We treat this variable as population, from which we draw multiple samples. Our goal here is to estimate the population mean.


First, we can check the population distribution: 

```{r, echo = TRUE}

pop <- long.data$skalo.true
pop.mean <- mean(pop)
pop.var <- mean((pop-pop.mean)^2)

hist(pop,freq=F)
```

The distribution is multi-modal and skewed. The population mean is `r round(pop.mean,3)` and the population variance is `r round(pop.var,3)``


From this population, we can draw multiple samples with size of 2, calculate the sample sum and observe its distribution.

```{r, echo = TRUE}
n.iter <- 5000
sample.size <- 2
all.sample.sum <- rep(NA,n.iter)
for (i in 1:n.iter){
  this.sample <- sample(pop,size=sample.size,replace=TRUE)
  this.sample.sum <- sum(this.sample)
  all.sample.sum[i] <- this.sample.sum
}

plot(density(all.sample.sum),
     main=paste0("Sum of the random draws (n=",sample.size,")"))
mean(all.sample.sum)
var(all.sample.sum)

```

From this population, we can draw multiple samples with size of 10, calculate the sample sum and observe its distribution.

```{r, echo = TRUE}
sample.size <- 10
all.sample.sum <- rep(NA,n.iter)
for (i in 1:n.iter){
  this.sample <- sample(pop,size=sample.size,replace=TRUE)
  this.sample.sum <- sum(this.sample)
  all.sample.sum[i] <- this.sample.sum
}

plot(density(all.sample.sum),
     main=paste0("Sum of the random draws (n=",sample.size,")"))
mean(all.sample.sum)
var(all.sample.sum)

```

From this population, we can draw multiple samples with size of 30, calculate the sample sum and observe its distribution.

```{r, echo = TRUE}
sample.size <- 30
all.sample.sum <- rep(NA,n.iter)
for (i in 1:n.iter){
  this.sample <- sample(pop,size=sample.size,replace=TRUE)
  this.sample.sum <- sum(this.sample)
  all.sample.sum[i] <- this.sample.sum
}

plot(density(all.sample.sum),
     main=paste0("Sum of the random draws (n=",sample.size,")"))
mean(all.sample.sum)
var(all.sample.sum)
```

From the population, we can draw multiple samples with size of 100, calculate the sample sum and observe its distribution.

```{r, echo = TRUE}
sample.size <- 100
all.sample.sum <- rep(NA,n.iter)
for (i in 1:n.iter){
  this.sample <- sample(pop,size=sample.size,replace = TRUE)
  this.sample.sum <- sum(this.sample)
  all.sample.sum[i] <- this.sample.sum
}

plot(density(all.sample.sum),
     main=paste0("Sum of the random draws (n=",sample.size,")"))
mean(all.sample.sum)
var(all.sample.sum)
```


From the population, we can draw multiple samples with size of 500, calculate the sample sum and observe its distribution.

```{r, echo = TRUE}
sample.size <- 500
all.sample.sum <- rep(NA,n.iter)
for (i in 1:n.iter){
  this.sample <- sample(pop,size=sample.size,replace=TRUE)
  this.sample.sum <- sum(this.sample)
  all.sample.sum[i] <- this.sample.sum
}

plot(density(all.sample.sum),
     main=paste0("Sum of the random draws (n=",sample.size,")"))
par(new=T)
this.norm <- function(x) dnorm(x,
                               mean=pop.mean*sample.size,
                               sd=sqrt(sample.size*pop.var))
curve(this.norm,add=T,col="blue",lty=2)
mean(all.sample.sum)
var(all.sample.sum)
```


Above, with increasing number of observations, the distribution of the sample sum becomes closer to a normal distribution. If we draw an infinitely large number of samples with a certain size, the distribution converges to a normal distribution whose mean is identical with the population mean times sample size and variance  with the population variance times the sample size: $$\sum x \sim N(n\mu , n\sigma^2)$$. If we divide the mean and variance of the above distribution (`r round(mean(all.sample.sum),3)` and `r round(naive.var(all.sample.sum),3)`) by n (`r sample.size`), we obtain `r round(mean(all.sample.sum)/sample.size,3)` and `r round(naive.var(all.sample.sum)/sample.size,3)`. These are very close to the population mean and variance (`r round(pop.mean,3)` and `r round(pop.var,3)`)


If you divide the sample sum by the sample size, you will obtain the sample mean. In the above figure, we always had the same sample size (n=500), therefore the sample mean also has the same form of the distribution, which also converges to a normal distribution.


```{r}
all.sample.mean <- all.sample.sum/sample.size
plot(density(all.sample.mean),
     main=paste0("Mean of the random draws (n=",sample.size,")"))
par(new=T)
this.norm <- function(x) dnorm(x,
                               mean=pop.mean,
                               sd=sqrt(pop.var/sample.size))
curve(this.norm,add=T,col="blue",lty=2)
```


The mean and variance of this distribution are `r round(mean(all.sample.mean),3)` and `r round(naive.var(all.sample.mean),3)`. This should follow the following: $$\bar{x} \sim N(\mu , \sigma^2 / n)$$.





## Regressing like/dislike of parties on left-right ideological distance and log response time.


We first recap the simple regression model, which we estimated above:


```{r}
summary(lm.out.abs <- lm((long.data$skalo.true ~ long.data$lr.dist.abs)))

```

You think that the time to respond to the questions can affect party evaluation, as well. That is, those who quickly answer may differ from those who deliberate in answering.

First, you can check the distribution of the time:

```{r}
hist(long.data$time, 
     main="",xlab="Response time (in sec)",
     freq=FALSE)
```

You see some outliers who needed much longer time to respond the questions than the others. To reduce the impact of such outliers, you create a new variably by  logarithmizing the original variable:

```{r}

long.data$log.time <- log(long.data$time) 

hist(long.data$log.time, 
     main="",xlab="Response time (in log(sec))",
     freq=FALSE)
```

Now you include the log response time into the regression analysis:


```{r}


summary(lm.out.abs.time <- 
          lm(skalo.true ~ lr.dist.abs + log.time,
             data=long.data))


```

The new variable, log response time, has a positive effect. However, the effect is not significant at 5% level.  

To interpret the slope coefficient of a logarithmized variable is tricky.According to the regression result above, unit increase of log response time is associated with increase of `r coef(lm.out.abs.time)[3]`. However, a unit increase of the log response time is not constant on the scale of the raw response time.

To see what a unit increase of the log response time means, we can first check the logarithm function.

```{r}
curve(log,0,70,axes=F)
axis(1,at=exp(seq(0,5)),
     c("",round(exp(seq(1,5)),2))
     )
axis(2,at=seq(0,5))

for (i in 1:4){
  lines(c(0,exp(i)),rep(i,2),lty=2)
  lines(rep(exp(i),2),c(0,i),lty=2)
}

```

Note that the opposite function of log() is the exponential function (exp()). To solve e.g. log(x)=2 for x, we can calculate just exp(2)=`r exp(2)`.

The above figure clearly demonstrates that increase from log(2) to log(3) and that from log(3) to log(4) correspond different increase in the raw response time. If you however take the growth, it is constant. That is:

- exp(3)/exp(2) = `r round(exp(3),2)`/`r round(exp(2),2)` = `r exp(3)/exp(2)`
- exp(4)/exp(3) = `r round(exp(4),2)`/`r round(exp(3),2)` = `r exp(4)/exp(3)`

That is, we can interpret the coefficient of the log response time (`r coef(lm.out.abs.time)[3]`) as change in Y given X grows constantly. More specifically, given a response time is 1% longer, the evaluation will change with `r coef(lm.out.abs.time)[3]`/100.


Consequently, the result will not change if we take the log of response time in minutes instead of in seconds:

```{r}
long.data$time.min <- long.data$time/60

long.data$log.time.min <- log(long.data$time.min)

head(long.data[,c("time","log.time","time.min","log.time.min")],n=10)


summary(lm.out.abs.time.min <- 
          lm(skalo.true ~ lr.dist.abs + log.time.min,
             data=long.data))

```


Concerning the estimated effect of the left-right distance, it is almost identical with that in the previous model with only one independent variable: `r coef(lm.out.abs)[2]`. This is because there is alsmot zero covariance between both independent variables. Corresnpondingly, the estimated effect is almost zero if you regress the log response time on the left-right distance:


```{r}

naive.cov(long.data$log.time , long.data$lr.dist.abs)

summary(lm(log.time ~ lr.dist.abs,data=long.data))
```

That is, there is no omitted variable bias due to the response time variable. Note that this does not necessarily mean that there does not exist any omitted variable bias, at all. Potentially there can be further important variable. 


## Regressing like/dislike of parties on left-right ideological distance, the squared distance and log response time.

We now extend the above multiple regression models with the squared distance (lr.dist.sqr). 


```{r}

summary(lm.out.abs.sqr.time <- 
          lm(skalo.true ~ lr.dist.abs +  lr.dist.sqr + log.time ,
             data=long.data))

```

You have to be able to test the corresponding hypotheses in one-sided and two-sided manners.


## t distribution for one-sided and two-sided alternatives

```{r}

x.scale <- seq(-3,3,by=0.01)
this.df <- lm.out.abs.sqr.time$df.residual
this.y <- dt(x.scale,df=this.df)

par(mfrow=c(1,2))

for (zweiseitig in c(FALSE,TRUE)){
  plot(x.scale,this.y,type="l",xlab="t",ylab="Density",
       main=paste("t-distribution with df=",this.df))

  if (zweiseitig) {this.95 <- qt(c(.025,.975),df=this.df)
       } else {
       this.95 <- qt(c(.95),df=this.df)
     }
  for (i in 1:length(this.95)){
    lines(rep(this.95[i],2),c(0,dt(this.95[i],df=this.df)),lty=2)
    text(this.95[i],0,pos=2,round(this.95[i],3))
  }
  text(0,0.2,"95%")
}


```




## Constructing the 95% confidence interval

Obtain the point estimates and standard errors.

```{r}

point.estimates <- coefficients(summary(lm.out.abs.sqr.time))[,1]
standard.errors <- coefficients(summary(lm.out.abs.sqr.time))[,2]
```

Compute the critical value.

```{r}
c.95 <- qt(0.975,df= lm.out.abs.sqr.time$df.residual)
c.95
```

Calculate the upper and lower bounds of confidence intervals.

```{r}
upperb <- point.estimates + standard.errors * c.95 
lowerb <- point.estimates + standard.errors * c.95 *-1

cbind(point.estimates,standard.errors,
      lowerb,upperb)
```


## Is the effect of log reponse time larger than that of the squared distance?

To do this, you have to set up another variable by summing both independent variables.  

```{r}
long.data$time.sqr <- long.data$log.time + long.data$lr.dist.sqr
```

Replace the squared distance  variable with the new variable.

```{r}
summary(lm.out.abs.sqr.time.2 <- 
          lm(skalo.true ~ lr.dist.abs + log.time + time.sqr ,
             data=long.data))
```

The effect of "log.time" is the difference of the effect of "log.time" and "squared distance" in the original regression. The output tells that the difference is not significant at 5% level.



## Testing against the null hypothesis that "log.time" AND "lr.dist.sqr" have no effect on Y.

To test whether "log.time" AND "lr.dist.sqr" variables have no effect on the dependent variable, We first estimate the restricted model without the above variables.

```{r}
summary(lm.out.res <- 
          lm(skalo.true ~ lr.dist.abs ,
             data=long.data))
```
 
Subsequently, we calculate F value by using SSR.
 
```{r}
 
SSR.ur <- sum(lm.out.abs.sqr.time$residuals^2)
SSR.r <-  sum(lm.out.res$residuals^2)
q <- lm.out.res$df.residual - lm.out.abs.sqr.time$df.residual

F <- ((SSR.r - SSR.ur)/q)/(SSR.ur/lm.out.abs.sqr.time$df.residual)
F
```

 The same value can be also obtained  by using $R^2$.
 
```{r}

R2.ur <- summary(lm.out.abs.sqr.time)$r.squared
R2.r <- summary(lm.out.res)$r.squared


F <- ((R2.ur - R2.r)/q)/((1-R2.ur)/lm.out.abs.sqr.time$df.residual)
F 

```

We now compare the F value with the F distribution. 
 

```{r}

x.scale <- seq(0,10,by=0.01)
this.y <- df(x.scale,df1=q,df2=lm.out.abs.sqr.time$df.residual)

plot(x.scale,this.y,type="l",xlab="t",ylab="Density",
       main=paste("F-distribution with df=",q,"and",lm.out.abs.sqr.time$df.residual))

this.95 <- qf(c(.95),df1=q,df2=lm.out.abs.sqr.time$df.residual)
abline(v=this.95,lty=2)
text(1,0.2,"95%")
text(this.95,0,pos=2,round(this.95,3))
text(this.95,0.2,"Rejection region",pos=4)


``` 
 
Since the above calculated F is in the rejection region, we can reject the null hypothesis. 




## Confidence intervals for predictions

Let's turn to the original question whether ideological distance affects party evaluation. We have already estimated the simple regression model:

```{r}
summary(lm.out.abs <- lm((long.data$skalo.true ~ long.data$lr.dist.abs)))
```

```{r}
plot(long.data$skalo.true ~ long.data$lr.dist.abs,
     xlab="Ideological Distance (abs)",
     ylab="Like/Dislike (true value)")
abline(lm.out.abs)
```

The regression line represents the expected value given certain $x$ values. Now, we wish to obtain the standard error of the expected value given $x=c$. For this purpose, we can just regress $y$ on $x-c$.

For example, the expected value for ideological distance with 5 units  has the following standard error:

```{r}
  new.x <- long.data$lr.dist.abs - 5
  lm.out <- lm(skalo.true ~ new.x,data=long.data)
  summary(lm.out)$coefficients

```

The standard error of the intercept `r round(coef(lm.out)[1],3)` is the standard error of the expected value.

We repeat the same exercise for the x values between 0 and 10.

We first calculate the standard errors for all x values:

```{r}
x.values <- seq(0,10,by=0.1)

all.se <- rep(NA,length(x.values))
for (i in 1:length(x.values)){
  new.x <- long.data$lr.dist.abs - x.values[i]
  lm.out <- lm(skalo.true ~ new.x,data=long.data)
  all.se[i] <- summary(lm.out)$coefficients[1,2]
}

plot(x.values,all.se,xlab="Ideological distance",ylab="Standard error")

```

It is obvious that the standard errors of expected values are not constant, but they are larger for extreme values in $x$. In contrast, the smallest standard error is around 3. More precisely, it is at x=`r round(mean(long.data$lr.dist.abs),3)`, which is the mean value of x.

We build the confidence intervals by using the calculated standard errors:

```{r}
predicted <- coefficients(lm.out.abs)[1] + coefficients(lm.out.abs)[2]*x.values

critical.value <- qt(0.975,df=lm.out.abs$df.residual)

upper.bounds <- predicted + critical.value*all.se
lower.bounds <- predicted - critical.value*all.se

plot(long.data$skalo.true ~ long.data$lr.dist.abs,ylab="Party evaluation",xlab="Ideological distance",
            ylim=range(long.data$skalo.true,na.rm=T),
            xlim=range(long.data$lr.dist.abs,na.rm=T))
abline(lm.out.abs)
par(new=T)
plot(x.values,upper.bounds,ann=F,xlab="",ylab="",axes=F,col="red",type="l",
            ylim=range(long.data$skalo.true,na.rm=T),
            xlim=range(long.data$lr.dist.abs,na.rm=T))
par(new=T)
plot(x.values,lower.bounds,ann=F,xlab="",ylab="",axes=F,col="red",type="l",
            ylim=range(long.data$skalo.true,na.rm=T),
            xlim=range(long.data$lr.dist.abs,na.rm=T))

```

Above, we built confidence intervals for the expected values of y given x. We can also build the confidence interval for y given x (prediction interval). The variance of y given x consists of the estimated error variance ($\hat{\sigma}^2$) and the standard error above. 

```{r}
predict.var <- all.se + summary(lm.out.abs)$sigma^2
predict.se <- sqrt(predict.var)

critical.value <- qt(0.975,df=lm.out.abs$df.residual)

upper.bounds <- predicted + critical.value*predict.se
lower.bounds <- predicted - critical.value*predict.se

plot(long.data$skalo.true ~ long.data$lr.dist.abs,ylab="Party evaluation",xlab="Ideological distance",
            ylim=range(long.data$skalo.true,na.rm=T),
            xlim=range(long.data$lr.dist.abs,na.rm=T))
abline(lm.out.abs)
par(new=T)
plot(x.values,upper.bounds,ann=F,xlab="",ylab="",axes=F,col="red",type="l",
            ylim=range(long.data$skalo.true,na.rm=T),
            xlim=range(long.data$lr.dist.abs,na.rm=T))
par(new=T)
plot(x.values,lower.bounds,ann=F,xlab="",ylab="",axes=F,col="red",type="l",
            ylim=range(long.data$skalo.true,na.rm=T),
            xlim=range(long.data$lr.dist.abs,na.rm=T))

```




## Including dummy variables of parties as predictors

To see whether the ideological distance affects party evaluation, we have already estimated the following model for many times:

```{r }
summary(lm.out.abs <- lm(skalo.true ~ lr.dist.abs,data=long.data))


plot(long.data$skalo.true ~ long.data$lr.dist.abs,
     xlab="Ideological Distance (abs)",
     ylab="Like/Dislike (true value)")
abline(lm.out.abs)

```

While we have evaluation of 7 different parties in the data set, the model above assumes the same relationship between the ideological distance and party evaluation for all parties. In other words, the intercept and slope (and errors) are common for all parties. This is a quite restricted assumption. It is more reasonable to assume that the intercept varies among parties. It is because there can be various factors behind evaluation of different parties, which can result in different intercepts. The intercept is in our context the average evaluation of a party for the respondents with the same ideological position of the party at stake.

To do this, we can first create dummy variables for individual parties:

```{r}

long.data$dummy.cdu <- ifelse(long.data$party==1,1,0)
long.data$dummy.csu <- ifelse(long.data$party==2,1,0)
long.data$dummy.spd <- ifelse(long.data$party==3,1,0)
long.data$dummy.gru <- ifelse(long.data$party==4,1,0)
long.data$dummy.fdp <- ifelse(long.data$party==5,1,0)
long.data$dummy.afd <- ifelse(long.data$party==6,1,0)
long.data$dummy.lin <- ifelse(long.data$party==7,1,0)

# check the dummy variable

head(long.data,n=10)

```

Now we can check whether evaluation of CDU is different from that of the other parties:

```{r}
summary(lm.out.cdu <- lm(skalo.true ~ lr.dist.abs + dummy.cdu,data=long.data))

```

The results can be visualized as follows:

```{r}

plot(long.data$skalo.true ~ long.data$lr.dist.abs,
     xlab="Close <- -> Distant",ylab="Dislike <-  -> Like",
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
abline(lm.out.abs,     lty=2,lwd=1)

predicted.cdu.0 <- coef(lm.out.cdu)[1] + coef(lm.out.cdu)[2]*c(0:10)
predicted.cdu.1 <- coef(lm.out.cdu)[1] + coef(lm.out.cdu)[2]*c(0:10) + coef(lm.out.cdu)[3]

par(new=T)
plot(predicted.cdu.0 ~ c(0:10),col="grey",type="l",
     ann=F,xlab="",ylab="",axes=F,
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.cdu.1 ~ c(0:10),col="orange",type="l",
     ann=F,xlab="",ylab="",axes=F,
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
legend("topright",col=c("black","orange","gray"),
       lty=c(2,1,1),c("All parties","CDU","Other parties"),
       bty="n")


```

We can find the intercept for CDU is slightly lower than the other parties. Its difference is however so small that it can be due to by chance.

We can now extend the model with the intercept for CSU:


```{r}

summary(lm.out.cdu.csu <- lm(skalo.true ~ lr.dist.abs + 
           dummy.cdu + dummy.csu ,
           data=long.data))

```

```{r}

plot(long.data$skalo.true ~ long.data$lr.dist.abs,
     xlab="Close <- -> Distant",ylab="Dislike <-  -> Like",
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
abline(lm.out.abs,     lty=2,lwd=1)

predicted.others <- coef(lm.out.cdu.csu)[1] + coef(lm.out.cdu.csu)[2]*c(0:10)
predicted.cdu <- coef(lm.out.cdu.csu)[1] + coef(lm.out.cdu.csu)[2]*c(0:10) + coef(lm.out.cdu.csu)[3]
predicted.csu <- coef(lm.out.cdu.csu)[1] + coef(lm.out.cdu.csu)[2]*c(0:10) + coef(lm.out.cdu.csu)[4]

par(new=T)
plot(predicted.others ~ c(0:10),col="grey",type="l",
     ann=F,xlab="",ylab="",axes=F,
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.cdu ~ c(0:10),col="orange",type="l",
     ann=F,xlab="",ylab="",axes=F,
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.csu ~ c(0:10),col="cyan",type="l",
     ann=F,xlab="",ylab="",axes=F,
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))

legend("topright",col=c("black","orange","cyan","gray"),
       lty=c(2,1,1,1),c("All parties","CDU","CSU","Other parties"),
       bty="n")

```

Here, the CSU's intercept is lower than the other parties and it is significant at 5% level.

To obtain the intercept for all parties, you regress the party evaluation on the left-right distance and all party dummy variables:


```{r}

summary(lm.out.all <- lm(skalo.true ~ lr.dist.abs + 
           dummy.cdu + dummy.csu + dummy.spd +
             dummy.gru + dummy.fdp + dummy.afd + dummy.lin,
           data=long.data))

```

The effect of the dummy variable for the Linke was not estimated. This is due to the perfect collinearity (the dummy variable trap). What is happening here can be seen in the following plot:



```{r}

plot(long.data$skalo.true ~ long.data$lr.dist.abs,
     xlab="Close <- -> Distant",ylab="Dislike <-  -> Like",
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
abline(lm.out.abs,     lty=2,lwd=2)

predicted.others <- coef(lm.out.all)[1] + coef(lm.out.all)[2]*c(0:10)
predicted.cdu <- coef(lm.out.all)[1] + coef(lm.out.all)[2]*c(0:10) + coef(lm.out.all)[3]
predicted.csu <- coef(lm.out.all)[1] + coef(lm.out.all)[2]*c(0:10) + coef(lm.out.all)[4]
predicted.spd <- coef(lm.out.all)[1] + coef(lm.out.all)[2]*c(0:10) + coef(lm.out.all)[5]
predicted.gru <- coef(lm.out.all)[1] + coef(lm.out.all)[2]*c(0:10) + coef(lm.out.all)[6]
predicted.fdp <- coef(lm.out.all)[1] + coef(lm.out.all)[2]*c(0:10) + coef(lm.out.all)[7]
predicted.afd <- coef(lm.out.all)[1] + coef(lm.out.all)[2]*c(0:10) + coef(lm.out.all)[8]
predicted.lin <- coef(lm.out.all)[1] + coef(lm.out.all)[2]*c(0:10) + coef(lm.out.all)[9]

par(new=T)
plot(predicted.others ~ c(0:10),col="grey",type="l",
     ann=F,xlab="",ylab="",axes=F,
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.cdu ~ c(0:10),col="orange",type="l",
     ann=F,xlab="",ylab="",axes=F,
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.csu ~ c(0:10),col="cyan",type="l",
     ann=F,xlab="",ylab="",axes=F,
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.spd ~ c(0:10),col="red",type="l",
     ann=F,xlab="",ylab="",axes=F,
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.gru ~ c(0:10),col="green",type="l",
     ann=F,xlab="",ylab="",axes=F,
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.fdp ~ c(0:10),col="yellow",type="l",
     ann=F,xlab="",ylab="",axes=F,
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.afd ~ c(0:10),col="blue4",type="l",
     ann=F,xlab="",ylab="",axes=F,
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.lin ~ c(0:10),col="pink",type="l",
     ann=F,xlab="",ylab="",axes=F,
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))

legend("topright",col=c("black","orange","cyan","red","green","yellow","blue4","pink","gray"),
       lty=c(2,rep(1,8)),c("All parties","CDU","CSU","SPD","Greens","FDP","AfD","Linke",
                        "Other parties"),
       lwd=c(2,rep(1,8)),
       bty="n")

```

In the figure, you can see the regression lines for all parties except for the Linke. However, you see the gray line for the others. And this corresponds to the regression line for the linke. 

The equivalent model can be also estimated as follows:


```{r}
summary(lm.out.all2 <- lm(skalo.true ~ lr.dist.abs + as.factor(party),data=long.data))

```

In this model, the effect of the CDU-dummy was dropped. But the result is completely equivalent to the last model.


Now, we can observe the residuals to check the zero conditional mean:

```{r}
par(mfrow=c(1,2))
resid.1 <- residuals(lm.out.abs)
plot(resid.1 ~ long.data$lr.dist.abs,
     ylim=range(resid.1),xlim=c(0,10),
     ylab="Residuals",xlab="Distance",
     main="Model without party dummies")
E.u <- tapply(resid.1,as.factor(long.data$lr.dist.abs),mean)
par(new=T)
plot(E.u ~ c(0:10),
     ylim=range(resid.1),xlim=c(0,10),
     type="l",col="red",ann=F,xlab="",ylab="",axes=F)
abline(h=0,lty=2)

resid.5 <- residuals(lm.out.all2)
plot(resid.5 ~ long.data$lr.dist.abs,
     ylab="Residuals",xlab="Distance",
     main="Model with party dummies")
E.u <- tapply(resid.5,as.factor(long.data$lr.dist.abs),mean)
par(new=T)
plot(E.u ~ c(0:10),
     ylim=range(resid.5),xlim=c(0,10),
     type="l",col="red",ann=F,xlab="",ylab="",axes=F)
abline(h=0,lty=2)

```

Here, we still have a problem in the range for large ideological distance values.

## 7.4 Interactions involving dummy variables


Let's come back to the first model with the dummy variable for CDU evaluation:

```{r}
summary(lm.out.cdu)


```

While we estimated different intercepts for the evaluation of CDU and the other parties in the above model, we further assumed that the effect of the ideological distance is same for all parties. It is however not nescessarily true. The ideological distance can have different impacts on the evaluation of CDU and the other parties. To check this, we include now the interaction effect of both variables:

```{r}


summary(lm.out.cdu.int <- lm(skalo.true ~ lr.dist.abs * dummy.cdu,data=long.data))

```


This result is a bit tricky to interpret as you have already seen in a previous section. In the previous section, we visualized the results for ease of interpretation. This time, we conduct two separate regression analysis for CDU evaluation and the other evaluations. First we divide the dataset:

```{r}
cdu.dat <- long.data[long.data$party==1,]
non.cdu.dat <- long.data[long.data$party!=1,]

```

Subsequently, the simple regression model is estimated for the CDU evaluation...

```{r}
summary(lm.out.onlycdu <- lm(skalo.true ~ lr.dist.abs,data = cdu.dat))

```

... and the others.

```{r}
summary(lm.out.onlynoncdu <- lm( skalo.true ~ lr.dist.abs,data = non.cdu.dat))

```

The point estimates of both separate regression can be found in the first model, while the other elements (standard errors, degrees of freedom) are not always identical.


Now, we wish to test whether both regression models systematically differ from each other. The null-hypothesis is that both models are based on the same population model, which corresponds to the first model estimated without any interaction terms. We call the latter model the pooled model since it pools both groups into one dataset.

Here, the separated models constitute together an unrestricted model while the pooled model is restricted since we restrict the effects of both independent variables being same for both groups.

This can be tested by F-statistic (which is called "Chow statistic"):

$F = \frac{SSR_P - (SSR_1 + SSR_2)/(k+1)}{(SSR_1 + SSR_2)/[n-2(k+1)]}$

```{r}
SSR.p <- sum(lm.out.abs$residuals^2)
SSR.1 <- sum(lm.out.onlycdu$residuals^2)
SSR.2 <- sum(lm.out.onlynoncdu$residuals^2)
k <- 2

sum.df <- lm.out.onlycdu$df.residual+lm.out.onlynoncdu$df.residual


F <- ((SSR.p - (SSR.1+SSR.2))/(k+1))/((SSR.1+SSR.2)/(sum.df))
F

```

```{r, echo = FALSE}

x.scale <- seq(0,10,by=0.01)
this.y <- df(x.scale,df1=k+1,df2=sum.df)

plot(x.scale,this.y,type="l",xlab="t",ylab="Density",
       main=paste("F-distribution with df=",k+1,"and",sum.df))

this.95 <- qf(c(.95),df1=k+1,df2=sum.df)
abline(v=this.95,lty=2)
text(1,0.2,"95%")
text(this.95,0,pos=2,round(this.95,3))
text(this.95,0.2,"Rejection region",pos=4)

``` 

We can not reject here the null-hypothesis. 



## Pooled Cross Sections 


We estimate the following model with the quadratic function:

```{r}
summary(lm.out.dummy <- lm(skalo.true ~ lr.dist.abs + lr.dist.sqr + as.factor(party)
                           ,data=long.data))
```


```{r}

plot(long.data$skalo.true ~ long.data$lr.dist.abs,
     xlab="Close <- -> Distant",ylab="Dislike <-  -> Like",
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))

predicted.cdu <- coef(lm.out.dummy)[1] + coef(lm.out.dummy)[2]*c(0:10)+ coef(lm.out.dummy)[3]* (c(0:10))^2
predicted.csu <- coef(lm.out.dummy)[1] + coef(lm.out.dummy)[2]*c(0:10) +coef(lm.out.dummy)[3]* (c(0:10))^2 +coef(lm.out.dummy)[4]
predicted.spd <- coef(lm.out.dummy)[1] + coef(lm.out.dummy)[2]*c(0:10) +coef(lm.out.dummy)[3]* (c(0:10))^2 +coef(lm.out.dummy)[5]
predicted.gru <- coef(lm.out.dummy)[1] + coef(lm.out.dummy)[2]*c(0:10) +coef(lm.out.dummy)[3]* (c(0:10))^2 +coef(lm.out.dummy)[6]
predicted.fdp <- coef(lm.out.dummy)[1] + coef(lm.out.dummy)[2]*c(0:10) +coef(lm.out.dummy)[3]* (c(0:10))^2 +coef(lm.out.dummy)[7]
predicted.afd <- coef(lm.out.dummy)[1] + coef(lm.out.dummy)[2]*c(0:10) +coef(lm.out.dummy)[3]* (c(0:10))^2 +coef(lm.out.dummy)[8]
predicted.lin <- coef(lm.out.dummy)[1] + coef(lm.out.dummy)[2]*c(0:10) +coef(lm.out.dummy)[3]* (c(0:10))^2 +coef(lm.out.dummy)[9]

par(new=T)
plot(predicted.cdu ~ c(0:10),col="orange",type="l",
     ann=F,axes=F,xlab="",ylab="",
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.csu ~ c(0:10),col="cyan",type="l",
     ann=F,axes=F,xlab="",ylab="",
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.spd ~ c(0:10),col="red",type="l",
     ann=F,axes=F,xlab="",ylab="",
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.gru ~ c(0:10),col="green",type="l",
     ann=F,axes=F,xlab="",ylab="",
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.fdp ~ c(0:10),col="yellow",type="l",
     ann=F,axes=F,xlab="",ylab="",
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.afd ~ c(0:10),col="blue4",type="l",
     ann=F,axes=F,xlab="",ylab="",
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.lin ~ c(0:10),col="pink",type="l",
     ann=F,axes=F,xlab="",ylab="",
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))


legend("topright",col=c("orange","cyan","red","green","yellow","blue4","pink"),
       lty=c(rep(1,8)),c("CDU","CSU","SPD","Greens","FDP","AfD",
                        "Linke"),
       lwd=c(rep(1,8)),
       bty="n")

```


Let's look at the data again:

```{r}

head(long.data[,1:7],n=20)

```

There is a variable "wave" which codes the time point of data collection:

```{r}
table(long.data$wave)
```

According to the frequency table, the data were collected at three different time points. That is, we pooled three cross sections. 



We might be interested whether the relationship between party evaluation and ideological distance is different at the three time points. One possibility is to estimate different regression models for each time point and compare them with the overall regression (the Chow test). Here, we add the interaction effects with the dummy variable for the panel waves. This is equivalent to estimate three separate regression models:



```{r}
summary(lm.out.chow <- lm(skalo.true ~ lr.dist.abs* as.factor(wave) + 
                             lr.dist.sqr* as.factor(wave) + 
                             as.factor(party)* as.factor(wave) ,data=long.data))
```

The result shows that multiple interaction effects with the 3rd wave is significant. That is, we find significant differences in the regression lines between the first/second waves and the third wave. Further, R-square is  larger than that of the model without dummy (`r round(summary(lm.out.dummy)$r.squared,4)'). It is also possible to calculate the exact F-statistics based on the R-square, which is large enough to reject the null hypothesis of no structural change across time.






## Panel data and fixed effect models

In the above analysis, we just treated both panel waves as independent (*independently pooled cross sections*). If one looks at the data more closely, this is not the case:

```{r}
head(long.data[24:100,1:7],n=20)
```

The first three digits of $id$ is the subject number and this indicates that the first 14 rows are of the same respondent. For each panel wave, we have 7 evaluations of the same individual. Therefore, we have here a panel dataset.

It is reasonable to assume that individual respondents use the scale for party evaluation differently. That is, some tend to evaluate parties more positively in general than others. To capture such an unobserved heterogeneity between individual subjects, we include the dummy variable for all respondents:

```{r}

long.data$subject <- round(long.data$id/100)

summary(lm.out.dummy.fe <- lm(skalo.true ~ lr.dist.abs +lr.dist.sqr + as.factor(party) + as.factor(wave) + as.factor(subject) ,data=long.data))

```


Since we have `r  length(unique(long.data$subject))' respondents, there are so many dummy variables and their coefficients. To get intuition, we can check two respondents (subject.id==120 and 162) in the graphical presentation:


```{r}

par(mfrow=c(1,2))
for (i.subj in c(120,162)){
  
  this.fe <-coef(lm.out.dummy.fe)[grep(as.character(i.subj),
                                       names(coef(lm.out.dummy.fe)))]
  
  plot(long.data$skalo.true ~ long.data$lr.dist.abs,
     xlab="Close <- -> Distant",ylab="Dislike <-  -> Like",
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T),col="grey",
     main=paste("Subject ID:",i.subj))
  par(new=T)  
  plot(long.data$skalo.true[long.data$subject==i.subj] ~
         long.data$lr.dist.abs[long.data$subject==i.subj],
     ann=F,axes=F,xlab="",ylab="",
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T),
     col="red",pch=19)

predicted.cdu <- coef(lm.out.dummy)[1] + coef(lm.out.dummy)[2]*c(0:10)+ coef(lm.out.dummy)[3]* (c(0:10))^2 + this.fe
predicted.csu <- coef(lm.out.dummy)[1] + coef(lm.out.dummy)[2]*c(0:10) +coef(lm.out.dummy)[3]* (c(0:10))^2 +coef(lm.out.dummy)[4] + this.fe
predicted.spd <- coef(lm.out.dummy)[1] + coef(lm.out.dummy)[2]*c(0:10) +coef(lm.out.dummy)[3]* (c(0:10))^2 +coef(lm.out.dummy)[5] + this.fe
predicted.gru <- coef(lm.out.dummy)[1] + coef(lm.out.dummy)[2]*c(0:10) +coef(lm.out.dummy)[3]* (c(0:10))^2 +coef(lm.out.dummy)[6] + this.fe
predicted.fdp <- coef(lm.out.dummy)[1] + coef(lm.out.dummy)[2]*c(0:10) +coef(lm.out.dummy)[3]* (c(0:10))^2 +coef(lm.out.dummy)[7] + this.fe
predicted.afd <- coef(lm.out.dummy)[1] + coef(lm.out.dummy)[2]*c(0:10) +coef(lm.out.dummy)[3]* (c(0:10))^2 +coef(lm.out.dummy)[8] + this.fe
predicted.lin <- coef(lm.out.dummy)[1] + coef(lm.out.dummy)[2]*c(0:10) +coef(lm.out.dummy)[3]* (c(0:10))^2 +coef(lm.out.dummy)[9] + this.fe

par(new=T)
plot(predicted.cdu ~ c(0:10),col="orange",type="l",
     ann=F,axes=F,xlab="",ylab="",
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.csu ~ c(0:10),col="cyan",type="l",
     ann=F,axes=F,xlab="",ylab="",
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.spd ~ c(0:10),col="red",type="l",
     ann=F,axes=F,xlab="",ylab="",
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.gru ~ c(0:10),col="green",type="l",
     ann=F,axes=F,xlab="",ylab="",
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.fdp ~ c(0:10),col="yellow",type="l",
     ann=F,axes=F,xlab="",ylab="",
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.afd ~ c(0:10),col="blue4",type="l",
     ann=F,axes=F,xlab="",ylab="",
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.lin ~ c(0:10),col="pink",type="l",
     ann=F,axes=F,xlab="",ylab="",
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))


legend("topright",col=c("orange","cyan","red","green","yellow","blue4","pink"),
       lty=c(rep(1,8)),c("CDU","CSU","SPD","Greens","FDP","AfD",
                        "Linke"),
       lwd=c(rep(1,8)),
       bty="n")
}



```

It is obvious that both respondents use the scale for like/dislike differently. Respondent 120 tend to evaluate parties more positively in general than Repondent 162.  


One can also check the correlation between ideological distances as independent variable and the estimated fixed effects:


```{r}
fixed.effects <- coef(lm.out.dummy.fe)[grep("subject",
                                       names(coef(lm.out.dummy.fe)))]
fixed.effects <- c(coef(lm.out.dummy.fe)[1],fixed.effects)
names(fixed.effects) <- unique(long.data$subject)

o <- match(long.data$subject,names(fixed.effects))

long.data$fixed.effects <- fixed.effects[o]


plot(long.data$fixed.effects ~ long.data$lr.dist.abs,
     ylab="Fixed effects",xlab="Ideological Distance (abs)")

cor.test(long.data$fixed.effects ,long.data$lr.dist.abs)
cor.test(long.data$fixed.effects ,long.data$lr.dist.sqr)


```

The estimated positive correlation is also significant at 5% level. Therefore, the model without the fixed effect is likely to violate the important assumption of the linear regression model.


### First differenced estimator

Another possibility to cope with the individual heterogeneity is to take the first-differences.

In our dataset, each of the respondent-party pairs may have two observations. For example, the first and eighth rows are Respondent 105's evaluation of CDU (party=1) at two different panel waves. 


```{r}
head(long.data[24:100,1:7],n=20)
```


We are now reoganizing the data which only include the adjacent respondent-party pairs and they are grouped in the next rows: 



```{r}



long.data$subject.party <- long.data$subject*10 + long.data$party

long.data <- long.data[order(long.data$subject.party),]

# keeping only adjacent data
freq.subject.party <- table(long.data$subject.party)
adjacent <- as.numeric(names(freq.subject.party[freq.subject.party>=2]))

adjacent.data <- long.data[long.data$subject.party %in% adjacent,]

head(adjacent.data[,1:7],n=20)

```




From this data, we create the first differences 




```{r}
# creating a wide format data
unique.subject.party <- unique(long.data$subject.party)

diff.data <- NULL
for (i.wave in 1:2){
  dat.t1 <- adjacent.data[adjacent.data$wave==i.wave,
                     c("subject.party","skalo.true",
                       "lr.dist.abs","lr.dist.sqr","party")] 
  dat.t2 <- adjacent.data[adjacent.data$wave==i.wave+1,
                     c("subject.party","skalo.true",
                       "lr.dist.abs","lr.dist.sqr","party")] 
  
  o <- match(dat.t1$subject.party,dat.t2$subject.party)
  
  this.diff.data <- dat.t2[o,] - dat.t1
  this.diff.data$subject.party <- dat.t1$subject.party
  this.diff.data$party <- dat.t1$party
  this.diff.data$wave <- i.wave+1
  this.diff.data <- this.diff.data[!is.na(this.diff.data$skalo.true),]
  
  diff.data <- rbind(diff.data,
                     this.diff.data
                     )
}


summary(lm(skalo.true ~ lr.dist.abs +lr.dist.sqr + as.factor(party),data=diff.data))


```

Now, the coefficients of both ideological distance measures are very small and only the that of the absolute ideological distance measure is significant at 5% more. Here, however, we have to note that we sweeped out individual heterogeneity by first differencing. This strongly reduced the variance of both dependent and independent variables (see the figure below). Furthermore, our independent variables are certainly suffering from measurement errors. The first differencing makes this problem even more serious.


```{r}
par(mfrow=c(1,2))

plot(long.data$skalo.true ~ long.data$lr.dist.abs,main="Rawdata",
     ylab="Party evaluation",xlab="Ideological distance")
plot(diff.data$skalo.true ~ diff.data$lr.dist.abs,main="First differences",
     ylab="Party evaluation",xlab="Ideological distance")
```


## Censored models


We have already estimated the following model for many times:

```{r}


summary(lm.out.abs <- lm(skalo.true ~ lr.dist.abs,data=long.data))



plot(long.data$skalo.true ~ long.data$lr.dist.abs,
     xlab="Ideological Distance (abs)",
     ylab="Like/Dislike (true value)")
abline(lm.out.abs)

```

In the very early session, we created the dependent variable under the assumption of random measurement errors from a survey item. If we use the original survey variable, you will obtain the following results:


```{r}

summary(lm.out.abs0 <- lm(skalo ~ lr.dist.abs,data=long.data))



plot(jitter(long.data$skalo) ~ long.data$lr.dist.abs,
     xlab="Ideological Distance (abs)",
     ylab="Like/Dislike (true value)")
abline(lm.out.abs0)

```

The original variable of party evaluation was measured by using a scale from -5 to +5. However, the predicted values for x=8 or larger are outside of the range. 

Now we can conceive that the party evaluation measure is censored. That is, we assume that among those who rated a party with +5, there are some respondents who would have rated the party with +6 or more. And the same can be also assumed for those who rated a party with -5. This idea is behind the censored regression models:


```{r}
library(censReg)

summary(cens.out.abs <- censReg(skalo ~ lr.dist.abs,
                                left=-5,right=5,
                                data=long.data))



```

The result of the censored model can be compared with the simple linear regression model in the following figure:



```{r}

plot(jitter(long.data$skalo) ~ long.data$lr.dist.abs,
     xlab="Ideological Distance (abs)",
     ylab="Like/Dislike (true value)")
abline(lm.out.abs0)
abline(coef=cens.out.abs$estimate[1:2],col="red")
legend("topright",lty=1,
       col=c("black","red"),
       legend=c("Linear regression model","Censored model"),
       bty="n")

```

The difference is in particular clear in the region with a large ideological distance (x>6). While the censored model assumes smaller values behind the measurement of y=-5, the linear regression model takes all the measurement value as face value. Therefore, the latter is more flat to minimize the residuals. In contrast, the slope of the censored model is steeper since the range of the dependent variable does not play a  role.

We can now extend the model with the party-dummy variables to consider the heterogeneity among parties:


```{r}

summary(cens.out.dummy <- censReg(skalo ~ lr.dist.abs+as.factor(party),
                                left=-5,right=5,
                                data=long.data))


```

We can further include the quadratic function:


```{r}

summary(cens.out.dummy.quad <- censReg(skalo ~ lr.dist.abs+
                                  lr.dist.sqr +
                                  as.factor(party),
                                left=-5,right=5,
                                data=long.data))


```

Interestingly, the coefficient of the quadratic distance is not significant (at 5%-level). If we compare this model with the last one in the log-likelihood, the improvement is very small and it is not significant at 5%-level.

This result is reasonable if we consider that the linear model with the quadratic term lead to a convex function. This convex function predicted higher values in party evaluation for those with larger ideological distances. This was an artifact due to the censored dependent variable.

Further, we can extend the previous model (without the quadratic distance) by including the fixed effects for respondents:



```{r}

summary(cens.out.dummy.fe <- censReg(skalo ~ lr.dist.abs+as.factor(party)+
                                    as.factor(subject),
                                left=-5,right=5,
                                data=long.data))


```


We can check whether we should include the fixed effect by using the likelihood ratio test. First, we check the log-likelihood values of both models:


```{r}

print('Model without FE')
logLik(cens.out.dummy)


print('Model with FE')
logLik(cens.out.dummy.fe)



```

We calculate the log-likelihood ratio and compare it with the chi-square distribution


```{r}

print('log likelihood ratio')
-2*(as.numeric(logLik(cens.out.dummy)) -
as.numeric(logLik(cens.out.dummy.fe)) ) 

print('critical value for p=0.95')
qchisq(0.95,df=
(attributes(logLik(cens.out.dummy.fe))$df-
                                            attributes(logLik(cens.out.dummy))$df)
)


```


Since the log-likelihood ratio exceeds the critical value, we can reject the null-hypothesis that both models' are at the same level in their performance. In other words, the model with fixed effects has significantly better performance than the model without fixed effects.

Now we can check again the two respondents whose predicted values have already demonstrated in the non-censored linear model (see the section for the panel data analysis):


```{r}

par(mfrow=c(1,2))
for (i.subj in c(120,162)){
  
  this.fe <-coef(cens.out.dummy.fe)[grep(as.character(i.subj),
                                       names(coef(cens.out.dummy.fe)))]
  
  plot(long.data$skalo.true ~ long.data$lr.dist.abs,
     xlab="Close <- -> Distant",ylab="Dislike <-  -> Like",
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T),col="grey",
     main=paste("Subject ID:",i.subj))
  par(new=T)  
  plot(long.data$skalo.true[long.data$subject==i.subj] ~
         long.data$lr.dist.abs[long.data$subject==i.subj],
     ann=F,axes=F,xlab="",ylab="",
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T),
     col="red",pch=19)

predicted.cdu <- coef(cens.out.dummy.fe)[1] + coef(cens.out.dummy.fe)[2]*c(0:10)+  this.fe
predicted.csu <- coef(cens.out.dummy.fe)[1] + coef(cens.out.dummy.fe)[2]*c(0:10) +coef(cens.out.dummy.fe)[3] + this.fe
predicted.spd <- coef(cens.out.dummy.fe)[1] + coef(cens.out.dummy.fe)[2]*c(0:10) +coef(cens.out.dummy.fe)[4] + this.fe
predicted.gru <- coef(cens.out.dummy.fe)[1] + coef(cens.out.dummy.fe)[2]*c(0:10) +coef(cens.out.dummy.fe)[5] + this.fe
predicted.fdp <- coef(cens.out.dummy.fe)[1] + coef(cens.out.dummy.fe)[2]*c(0:10) +coef(cens.out.dummy.fe)[6] + this.fe
predicted.afd <- coef(cens.out.dummy.fe)[1] + coef(cens.out.dummy.fe)[2]*c(0:10) +coef(cens.out.dummy.fe)[7] + this.fe
predicted.lin <- coef(cens.out.dummy.fe)[1] + coef(cens.out.dummy.fe)[2]*c(0:10) +coef(cens.out.dummy.fe)[8] + this.fe

par(new=T)
plot(predicted.cdu ~ c(0:10),col="orange",type="l",
     ann=F,axes=F,xlab="",ylab="",
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.csu ~ c(0:10),col="cyan",type="l",
     ann=F,axes=F,xlab="",ylab="",
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.spd ~ c(0:10),col="red",type="l",
     ann=F,axes=F,xlab="",ylab="",
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.gru ~ c(0:10),col="green",type="l",
     ann=F,axes=F,xlab="",ylab="",
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.fdp ~ c(0:10),col="yellow",type="l",
     ann=F,axes=F,xlab="",ylab="",
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.afd ~ c(0:10),col="blue4",type="l",
     ann=F,axes=F,xlab="",ylab="",
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.lin ~ c(0:10),col="pink",type="l",
     ann=F,axes=F,xlab="",ylab="",
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))


legend("topright",col=c("orange","cyan","red","green","yellow","blue4","pink"),
       lty=c(rep(1,8)),c("CDU","CSU","SPD","Greens","FDP","AfD",
                        "Linke"),
       lwd=c(rep(1,8)),
       bty="n")
}



```



```{r}
knitr::knit_exit()
```





```{r}

this.dat <- long.data[long.data$party==1,]

time.ten <- floor(this.dat$time/20)

p.est <- tapply(this.dat$skalo,as.factor(time.ten),mean)

this.func <- function(x) {
  out <- ci.sample.mean(x)
  out <- c(out$lower.b,out$upper.b)
  }

int.est <- tapply(this.dat$skalo,as.factor(time.ten),this.func)

int.est

plot(p.est ~ as.numeric(names(p.est)))


table(time.ten)

```

