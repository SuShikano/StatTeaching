---
title: "Example regression analysis: Party preference and ideological orientation"
author: "Susumu Shikano"
date: "Last compiled at `r format(Sys.Date(), '%d. %B %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

source("https://raw.githubusercontent.com/SuShikano/StatTeaching/main/func/naive_var.R")
source("https://raw.githubusercontent.com/SuShikano/StatTeaching/main/func/ci_sample_mean.R")

```

## Substantive motivation

You are interested in what factor determines citizens' evaluation of political parties. More specifically, you are interested in citizens' like/dislike of parties. You are speculating ideological proximity/distance behind this kind of evaluation. That is, a citizen likes a party whose ideological position is closer to their positions than another party with a more distant position. 

To test your idea, you conducted online surveys with students visiting a lecture. To obtain the most important variables, that is, respondents' like/dislike and proximity to different parties, you asked the following questions:


```{r , echo=FALSE}
knitr::include_graphics("image/Question_Skalo.png")

knitr::include_graphics("image/Question_LR.png")

```


We can load the dataset and check its content:



```{r }
# Loading the data file
load("data/Lec_Statistics_Ideology_Data.RData")

# Check which object is loaded
ls()

# Check the inside of the data
head(long.data,n=10)


```

The data contains four variables.

- id: Identification number of responses
- wave: survey wave
* party: Which party is at stake?
    + 1: CDU; 2: CSU; 3: SPD; 4: Greens; 5: FDP; 6: AfD; 7: Linke
- skalo: Respondents' like/dislike of a party (+5 like/-5 dislike)
- lr.self: Respondents' self placement on the left-right ideology scale (+5 right most/-5 left most)
- lr: Respondents' placement of the party on the left-right ideology scale (+5 right most/-5 left most)
- time: Response time for the whole skalometer question page (in seconds)

In the dataset, we have `r nrow(long.data)` observations. Note that each observation corresponds to a response. Since a respondent can give multiple responses for different parties, s/he can appear many times in this dataset. 


## Univariate discriptive statistics

We can start with the respondents' like/dislike of parties. Below its distribution:

```{r }
plot(table(long.data$skalo),type="h",main="Like/Dislike",ylab="Frequency")

```

Assume that this measurement contains random errors which follow a normal distribution $N(0,0.09)$:

```{r}
error.dist.density <- function(x){
  dnorm(x,mean=0,sd=sqrt(0.09))
}
plot(error.dist.density,xlim=c(-2,2),
     main="Normal distribution: N(0,0.09)",
     ylab="Density",
     xlab="Measurement Error")
```

For each of 2504 observations, we generate random errors and compute the true value behind the measurement:

```{r}
set.seed(1234567) # Setting a seed so that the "random" errors can be replicated.
measurement.error <- rnorm(nrow(long.data),mean=0,sd=sqrt(0.09))

long.data$skalo.true <- long.data$skalo - measurement.error

par(mfrow=c(1,2))

plot(table(measurement.error),type="h",xlab="measurement error",ylab="Frequency")
plot(table(long.data$skalo.true),type="h",xlab="true value",ylab="Frequency")
```

This way of plot is not informative since both variables can take all real numbers and each value appears only once. That is, the measurement errors and true values here are continuous random variables. Instead of counting all possible values, we can group the values for certain ranges and plot their densities in  histograms.

```{r}

par(mfrow=c(1,2))

hist(measurement.error,xlab="measurement error",ylab="Density",freq=FALSE,
     main="Measurement error")
hist(long.data$skalo.true,xlab="true value",ylab="Density",freq=FALSE,
     main="True values")

```

We can compute some descriptive statistics of the true value. More specifically, we can obtain mean, variance and standard deviation.

```{r}
print("The number of valid observations")
print(valid.n.skalo <- length(long.data$skalo.true))

print("The mean response")
print(mean.skalo <- sum(long.data$skalo.true)/valid.n.skalo)

print("Variance of the responses")
print(var.skalo <- sum((long.data$skalo.true - mean.skalo)^2)/valid.n.skalo )

print("Standard deviation of the responses")
print(sd.skalo <- sqrt(var.skalo))

```

The above procedure is a bit tedious since you are programming hand-rolled. Fortunately, there are ready-made built-in functions in R:

```{r}
length(long.data$skalo.true)
mean(long.data$skalo.true)
var(long.data$skalo.true)
sd(long.data$skalo.true)
```

If you compare the results with those above, however, there are slight differences in variance and standard deviation. This is because `var` and `sd` functions compute the unbiased estimates. That is, you divide not by the number of observations (`r valid.n.skalo`), but $n-1$ (`r valid.n.skalo-1`) to obtain the variance.

To obtain just a variance (not an estimate), you should correct as follows:

```{r}
var(long.data$skalo.true)*(valid.n.skalo-1)/valid.n.skalo

```

In R, one can also define your own function for the naive variance:


```{r}
naive.var <- function(x){
  diff.to.mean <- x - mean(x)
  squared.diff <- diff.to.mean^2
  output <- mean(squared.diff)
  output
}

naive.var(long.data$skalo.true)
```

Below, we can also observe further variables.


```{r }
hist(long.data$lr.self,br=seq(-5.5,5.5,by=1),main="",
     xlab="Left-Right Self-Placement",
     freq=FALSE)

mean(long.data$lr.self)
naive.var(long.data$lr.self)
```




```{r }
hist(long.data$lr,br=seq(-5.5,5.5,by=1),
     xlab="Left-Right Placement of parties",main="",
     freq=FALSE)

mean(long.data$lr)
naive.var(long.data$lr)

```
## Bivariate relationship between the party placement and self-placement

Now we can inspect the joint distribution of the the left-right party-placement and self-placement. We can just plot the distribution:


```{r}
plot(long.data$lr ~ long.data$lr.self,
     xlab="Self-placement",ylab="Party placement")
```

This is however not so informative since both variables are discrete and many responses have the same value combination. Therefore, we cannot see here how many responses are behind different value combinations. Here, we can just add small random values to both measures and plot them again:  

```{r}
plot(jitter(long.data$lr) ~ jitter(long.data$lr.self),
     xlab="Self-placement",ylab="Party placement")
```

The relationship of both variables is not clear to see. We can now calculate two summary statistics: covariance and correlation:

```{r}
cov(long.data$lr , long.data$lr.self)
cor(long.data$lr , long.data$lr.self)
```

Here again, we have to be careful that `cov` gives the unbiased estimate. For simple covariance, you need to write your own function:

```{r}

naive.cov <- function(x,y){
  mean((x - mean(x))*(y - mean(y)))
}

naive.cov(long.data$lr , long.data$lr.self)

```



## Creating a new variable

We can now create new variables based on the existing variables:

* Difference between the party placement and self-placement
* Absolute value of the above difference
* Squared value of the above difference

```{r }

long.data$lr.dist.dif <- long.data$lr - long.data$lr.self
long.data$lr.dist.abs <- abs(long.data$lr.dist.dif)
long.data$lr.dist.sqr <- long.data$lr.dist.dif^2

```


```{r}
mean(long.data$lr.dist.dif)
naive.var(long.data$lr.dist.dif)
```
These values can be also obtained from the statistics of the variables, based on which the variable was created:


```{r}
mean(long.data$lr) - mean(long.data$lr.self)
naive.var(long.data$lr) + naive.var(long.data$lr.self) - 2*naive.cov(long.data$lr,long.data$lr.self)

```


## Bivariate relationship between interested variables

Now we can inspect the joint distribution of the like/dislike variable and the absolute distances. We can just again plot the distribution with small random numbers:


```{r }
plot(long.data$skalo.true ~ long.data$lr.dist.abs,
     xlab="Ideological Distance (abs)",
     ylab="Like/Dislike (true value)")

naive.cov(long.data$skalo.true, long.data$lr.dist.abs)

cor(long.data$skalo.true, long.data$lr.dist.abs)
```

Now, the same exercise for the squared distances:

```{r }
plot(long.data$skalo.true ~ long.data$lr.dist.sqr,
     xlab="Ideological Distance (sqr)",
     ylab="Like/Dislike (true value)")

naive.cov(long.data$skalo, long.data$lr.dist.sqr)

cor(long.data$skalo, long.data$lr.dist.sqr)
```


## Central limit theorem

In this section, we learn the central limit theorem by using the generated true response to party evaluation. We treat this variable as population, from which we draw multiple samples. Our goal here is to estimate the population mean.


First, we can check the population distribution: 

```{r, echo = TRUE}

pop <- long.data$skalo.true
pop.mean <- mean(pop)
pop.var <- mean((pop-pop.mean)^2)

hist(pop,freq=F)
```

The distribution is multi-modal and skewed. The population mean is `r round(pop.mean,3)` and the population variance is `r round(pop.var,3)``


From this population, we can draw multiple samples with size of 2, calculate the sample sum and observe its distribution.

```{r, echo = TRUE}
n.iter <- 5000
sample.size <- 2
all.sample.sum <- rep(NA,n.iter)
for (i in 1:n.iter){
  this.sample <- sample(pop,size=sample.size,replace=TRUE)
  this.sample.sum <- sum(this.sample)
  all.sample.sum[i] <- this.sample.sum
}

plot(density(all.sample.sum),
     main=paste0("Sum of the random draws (n=",sample.size,")"))
mean(all.sample.sum)
var(all.sample.sum)

```

From this population, we can draw multiple samples with size of 10, calculate the sample sum and observe its distribution.

```{r, echo = TRUE}
sample.size <- 10
all.sample.sum <- rep(NA,n.iter)
for (i in 1:n.iter){
  this.sample <- sample(pop,size=sample.size,replace=TRUE)
  this.sample.sum <- sum(this.sample)
  all.sample.sum[i] <- this.sample.sum
}

plot(density(all.sample.sum),
     main=paste0("Sum of the random draws (n=",sample.size,")"))
mean(all.sample.sum)
var(all.sample.sum)

```

From this population, we can draw multiple samples with size of 30, calculate the sample sum and observe its distribution.

```{r, echo = TRUE}
sample.size <- 30
all.sample.sum <- rep(NA,n.iter)
for (i in 1:n.iter){
  this.sample <- sample(pop,size=sample.size,replace=TRUE)
  this.sample.sum <- sum(this.sample)
  all.sample.sum[i] <- this.sample.sum
}

plot(density(all.sample.sum),
     main=paste0("Sum of the random draws (n=",sample.size,")"))
mean(all.sample.sum)
var(all.sample.sum)
```

From the population, we can draw multiple samples with size of 100, calculate the sample sum and observe its distribution.

```{r, echo = TRUE}
sample.size <- 100
all.sample.sum <- rep(NA,n.iter)
for (i in 1:n.iter){
  this.sample <- sample(pop,size=sample.size,replace = TRUE)
  this.sample.sum <- sum(this.sample)
  all.sample.sum[i] <- this.sample.sum
}

plot(density(all.sample.sum),
     main=paste0("Sum of the random draws (n=",sample.size,")"))
mean(all.sample.sum)
var(all.sample.sum)
```


From the population, we can draw multiple samples with size of 500, calculate the sample sum and observe its distribution.

```{r, echo = TRUE}
sample.size <- 500
all.sample.sum <- rep(NA,n.iter)
for (i in 1:n.iter){
  this.sample <- sample(pop,size=sample.size,replace=TRUE)
  this.sample.sum <- sum(this.sample)
  all.sample.sum[i] <- this.sample.sum
}

plot(density(all.sample.sum),
     main=paste0("Sum of the random draws (n=",sample.size,")"))
par(new=T)
this.norm <- function(x) dnorm(x,
                               mean=pop.mean*sample.size,
                               sd=sqrt(sample.size*pop.var))
curve(this.norm,add=T,col="blue",lty=2)
mean(all.sample.sum)
var(all.sample.sum)
```


Above, with increasing number of observations, the distribution of the sample sum becomes closer to a normal distribution. If we draw an infinitely large number of samples with a certain size, the distribution converges to a normal distribution whose mean is identical with the population mean times sample size and variance  with the population variance times the sample size: $$\sum x \sim N(n\mu , n\sigma^2)$$. If we divide the mean and variance of the above distribution (`r round(mean(all.sample.sum),3)` and `r round(naive.var(all.sample.sum),3)`) by n (`r sample.size`), we obtain `r round(mean(all.sample.sum)/sample.size,3)` and `r round(naive.var(all.sample.sum)/sample.size,3)`. These are very close to the population mean and variance (`r round(pop.mean,3)` and `r round(pop.var,3)`)


If you divide the sample sum by the sample size, you will obtain the sample mean. In the above figure, we always had the same sample size (n=500), therefore the sample mean also has the same form of the distribution, which also converges to a normal distribution.


```{r}
all.sample.mean <- all.sample.sum/sample.size
plot(density(all.sample.mean),
     main=paste0("Mean of the random draws (n=",sample.size,")"))
par(new=T)
this.norm <- function(x) dnorm(x,
                               mean=pop.mean,
                               sd=sqrt(pop.var/sample.size))
curve(this.norm,add=T,col="blue",lty=2)
```


The mean and variance of this distribution are `r round(mean(all.sample.mean),3)` and `r round(naive.var(all.sample.mean),3)`. This should follow the following: $$\bar{x} \sim N(\mu , \sigma^2 / n)$$.





## Regressing like/dislike of parties on left-right ideological distance and log response time.


We first recap the simple regression model, which we estimated above:


```{r}
summary(lm.out.abs <- lm((long.data$skalo.true ~ long.data$lr.dist.abs)))

```

You think that the time to respond to the questions can affect party evaluation, as well. That is, those who quickly answer may differ from those who deliberate in answering.

First, you can check the distribution of the time:

```{r}
hist(long.data$time, 
     main="",xlab="Response time (in sec)",
     freq=FALSE)
```

You see some outliers who needed much longer time to respond the questions than the others. To reduce the impact of such outliers, you create a new variably by  logarithmizing the original variable:

```{r}

long.data$log.time <- log(long.data$time) 

hist(long.data$log.time, 
     main="",xlab="Response time (in log(sec))",
     freq=FALSE)
```

Now you include the log response time into the regression analysis:


```{r}


summary(lm.out.abs.time <- 
          lm(skalo.true ~ lr.dist.abs + log.time,
             data=long.data))


```

The new variable, log response time, has a positive effect. However, the effect is not significant at 5% level.  

To interpret the slope coefficient of a logarithmized variable is tricky.According to the regression result above, unit increase of log response time is associated with increase of `r coef(lm.out.abs.time)[3]`. However, a unit increase of the log response time is not constant on the scale of the raw response time.

To see what a unit increase of the log response time means, we can first check the logarithm function.

```{r}
curve(log,0,70,axes=F)
axis(1,at=exp(seq(0,5)),
     c("",round(exp(seq(1,5)),2))
     )
axis(2,at=seq(0,5))

for (i in 1:4){
  lines(c(0,exp(i)),rep(i,2),lty=2)
  lines(rep(exp(i),2),c(0,i),lty=2)
}

```

Note that the opposite function of log() is the exponential function (exp()). To solve e.g. log(x)=2 for x, we can calculate just exp(2)=`r exp(2)`.

The above figure clearly demonstrates that increase from log(2) to log(3) and that from log(3) to log(4) correspond different increase in the raw response time. If you however take the growth, it is constant. That is:

- exp(3)/exp(2) = `r round(exp(3),2)`/`r round(exp(2),2)` = `r exp(3)/exp(2)`
- exp(4)/exp(3) = `r round(exp(4),2)`/`r round(exp(3),2)` = `r exp(4)/exp(3)`

That is, we can interpret the coefficient of the log response time (`r coef(lm.out.abs.time)[3]`) as change in Y given X grows constantly. More specifically, given a response time is 1% longer, the evaluation will change with `r coef(lm.out.abs.time)[3]`/100.


Consequently, the result will not change if we take the log of response time in minutes instead of in seconds:

```{r}
long.data$time.min <- long.data$time/60

long.data$log.time.min <- log(long.data$time.min)

head(long.data[,c("time","log.time","time.min","log.time.min")],n=10)


summary(lm.out.abs.time.min <- 
          lm(skalo.true ~ lr.dist.abs + log.time.min,
             data=long.data))

```


Concerning the estimated effect of the left-right distance, it is almost identical with that in the previous model with only one independent variable: `r coef(lm.out.abs)[2]`. This is because there is alsmot zero covariance between both independent variables. Corresnpondingly, the estimated effect is almost zero if you regress the log response time on the left-right distance:


```{r}

naive.cov(long.data$log.time , long.data$lr.dist.abs)

summary(lm(log.time ~ lr.dist.abs,data=long.data))
```

That is, there is no omitted variable bias due to the response time variable. Note that this does not necessarily mean that there does not exist any omitted variable bias, at all. Potentially there can be further important variable. 


## Regressing like/dislike of parties on left-right ideological distance, the squared distance and log response time.

We now extend the above multiple regression models with the squared distance (lr.dist.sqr). 


```{r}

summary(lm.out.abs.sqr.time <- 
          lm(skalo.true ~ lr.dist.abs +  lr.dist.sqr + log.time ,
             data=long.data))

```

You have to be able to test the corresponding hypotheses in one-sided and two-sided manners.


## t distribution for one-sided and two-sided alternatives

```{r}

x.scale <- seq(-3,3,by=0.01)
this.df <- lm.out.abs.sqr.time$df.residual
this.y <- dt(x.scale,df=this.df)

par(mfrow=c(1,2))

for (zweiseitig in c(FALSE,TRUE)){
  plot(x.scale,this.y,type="l",xlab="t",ylab="Density",
       main=paste("t-distribution with df=",this.df))

  if (zweiseitig) {this.95 <- qt(c(.025,.975),df=this.df)
       } else {
       this.95 <- qt(c(.95),df=this.df)
     }
  for (i in 1:length(this.95)){
    lines(rep(this.95[i],2),c(0,dt(this.95[i],df=this.df)),lty=2)
    text(this.95[i],0,pos=2,round(this.95[i],3))
  }
  text(0,0.2,"95%")
}


```




## Constructing the 95% confidence interval

Obtain the point estimates and standard errors.

```{r}

point.estimates <- coefficients(summary(lm.out.abs.sqr.time))[,1]
standard.errors <- coefficients(summary(lm.out.abs.sqr.time))[,2]
```

Compute the critical value.

```{r}
c.95 <- qt(0.975,df= lm.out.abs.sqr.time$df.residual)
c.95
```

Calculate the upper and lower bounds of confidence intervals.

```{r}
upperb <- point.estimates + standard.errors * c.95 
lowerb <- point.estimates + standard.errors * c.95 *-1

cbind(point.estimates,standard.errors,
      lowerb,upperb)
```


## Is the effect of log reponse time larger than that of the squared distance?

To do this, you have to set up another variable by summing both independent variables.  

```{r}
long.data$time.sqr <- long.data$log.time + long.data$lr.dist.sqr
```

Replace the squared distance  variable with the new variable.

```{r}
summary(lm.out.abs.sqr.time.2 <- 
          lm(skalo.true ~ lr.dist.abs + log.time + time.sqr ,
             data=long.data))
```

The effect of "log.time" is the difference of the effect of "log.time" and "squared distance" in the original regression. The output tells that the difference is not significant at 5% level.



## Testing against the null hypothesis that "log.time" AND "lr.dist.sqr" have no effect on Y.

To test whether "log.time" AND "lr.dist.sqr" variables have no effect on the dependent variable, We first estimate the restricted model without the above variables.

```{r}
summary(lm.out.res <- 
          lm(skalo.true ~ lr.dist.abs ,
             data=long.data))
```
 
Subsequently, we calculate F value by using SSR.
 
```{r}
 
SSR.ur <- sum(lm.out.abs.sqr.time$residuals^2)
SSR.r <-  sum(lm.out.res$residuals^2)
q <- lm.out.res$df.residual - lm.out.abs.sqr.time$df.residual

F <- ((SSR.r - SSR.ur)/q)/(SSR.ur/lm.out.abs.sqr.time$df.residual)
F
```

 The same value can be also obtained  by using $R^2$.
 
```{r}

R2.ur <- summary(lm.out.abs.sqr.time)$r.squared
R2.r <- summary(lm.out.res)$r.squared


F <- ((R2.ur - R2.r)/q)/((1-R2.ur)/lm.out.abs.sqr.time$df.residual)
F 

```

We now compare the F value with the F distribution. 
 

```{r}

x.scale <- seq(0,10,by=0.01)
this.y <- df(x.scale,df1=q,df2=lm.out.abs.sqr.time$df.residual)

plot(x.scale,this.y,type="l",xlab="t",ylab="Density",
       main=paste("F-distribution with df=",q,"and",lm.out.abs.sqr.time$df.residual))

this.95 <- qf(c(.95),df1=q,df2=lm.out.abs.sqr.time$df.residual)
abline(v=this.95,lty=2)
text(1,0.2,"95%")
text(this.95,0,pos=2,round(this.95,3))
text(this.95,0.2,"Rejection region",pos=4)


``` 
 
Since the above calculated F is in the rejection region, we can reject the null hypothesis. 




## Confidence intervals for predictions

Let's turn to the original question whether ideological distance affects party evaluation. We have already estimated the simple regression model:

```{r}
summary(lm.out.abs <- lm((long.data$skalo.true ~ long.data$lr.dist.abs)))
```

```{r}
plot(long.data$skalo.true ~ long.data$lr.dist.abs,
     xlab="Ideological Distance (abs)",
     ylab="Like/Dislike (true value)")
abline(lm.out.abs)
```

The regression line represents the expected value given certain $x$ values. Now, we wish to obtain the standard error of the expected value given $x=c$. For this purpose, we can just regress $y$ on $x-c$.

For example, the expected value for ideological distance with 5 units  has the following standard error:

```{r}
  new.x <- long.data$lr.dist.abs - 5
  lm.out <- lm(skalo.true ~ new.x,data=long.data)
  summary(lm.out)$coefficients

```

The standard error of the intercept `r round(coef(lm.out)[1],3)` is the standard error of the expected value.

We repeat the same exercise for the x values between 0 and 10.

We first calculate the standard errors for all x values:

```{r}
x.values <- seq(0,10,by=0.1)

all.se <- rep(NA,length(x.values))
for (i in 1:length(x.values)){
  new.x <- long.data$lr.dist.abs - x.values[i]
  lm.out <- lm(skalo.true ~ new.x,data=long.data)
  all.se[i] <- summary(lm.out)$coefficients[1,2]
}

plot(x.values,all.se,xlab="Ideological distance",ylab="Standard error")

```

It is obvious that the standard errors of expected values are not constant, but they are larger for extreme values in $x$. In contrast, the smallest standard error is around 3. More precisely, it is at x=`r round(mean(long.data$lr.dist.abs),3)`, which is the mean value of x.

We build the confidence intervals by using the calculated standard errors:

```{r}
predicted <- coefficients(lm.out.abs)[1] + coefficients(lm.out.abs)[2]*x.values

critical.value <- qt(0.975,df=lm.out.abs$df.residual)

upper.bounds <- predicted + critical.value*all.se
lower.bounds <- predicted - critical.value*all.se

plot(long.data$skalo.true ~ long.data$lr.dist.abs,ylab="Party evaluation",xlab="Ideological distance",
            ylim=range(long.data$skalo.true,na.rm=T),
            xlim=range(long.data$lr.dist.abs,na.rm=T))
abline(lm.out.abs)
par(new=T)
plot(x.values,upper.bounds,ann=F,xlab="",ylab="",axes=F,col="red",type="l",
            ylim=range(long.data$skalo.true,na.rm=T),
            xlim=range(long.data$lr.dist.abs,na.rm=T))
par(new=T)
plot(x.values,lower.bounds,ann=F,xlab="",ylab="",axes=F,col="red",type="l",
            ylim=range(long.data$skalo.true,na.rm=T),
            xlim=range(long.data$lr.dist.abs,na.rm=T))

```

Above, we built confidence intervals for the expected values of y given x. We can also build the confidence interval for y given x (prediction interval). The variance of y given x consists of the estimated error variance ($\hat{\sigma}^2$) and the standard error above. 

```{r}
predict.var <- all.se + summary(lm.out.abs)$sigma^2
predict.se <- sqrt(predict.var)

critical.value <- qt(0.975,df=lm.out.abs$df.residual)

upper.bounds <- predicted + critical.value*predict.se
lower.bounds <- predicted - critical.value*predict.se

plot(long.data$skalo.true ~ long.data$lr.dist.abs,ylab="Party evaluation",xlab="Ideological distance",
            ylim=range(long.data$skalo.true,na.rm=T),
            xlim=range(long.data$lr.dist.abs,na.rm=T))
abline(lm.out.abs)
par(new=T)
plot(x.values,upper.bounds,ann=F,xlab="",ylab="",axes=F,col="red",type="l",
            ylim=range(long.data$skalo.true,na.rm=T),
            xlim=range(long.data$lr.dist.abs,na.rm=T))
par(new=T)
plot(x.values,lower.bounds,ann=F,xlab="",ylab="",axes=F,col="red",type="l",
            ylim=range(long.data$skalo.true,na.rm=T),
            xlim=range(long.data$lr.dist.abs,na.rm=T))

```




## Including dummy variables of parties as predictors

To see whether the ideological distance affects party evaluation, we have already estimated the following model for many times:

```{r }
summary(lm.out.abs <- lm(skalo.true ~ lr.dist.abs,data=long.data))


plot(long.data$skalo.true ~ long.data$lr.dist.abs,
     xlab="Ideological Distance (abs)",
     ylab="Like/Dislike (true value)")
abline(lm.out.abs)

```

While we have evaluation of 7 different parties in the data set, the model above assumes the same relationship between the ideological distance and party evaluation for all parties. In other words, the intercept and slope (and errors) are common for all parties. This is a quite restricted assumption. It is more reasonable to assume that the intercept varies among parties. It is because there can be various factors behind evaluation of different parties, which can result in different intercepts. The intercept is in our context the average evaluation of a party for the respondents with the same ideological position of the party at stake.

To do this, we can first create dummy variables for individual parties:

```{r}

long.data$dummy.cdu <- ifelse(long.data$party==1,1,0)
long.data$dummy.csu <- ifelse(long.data$party==2,1,0)
long.data$dummy.spd <- ifelse(long.data$party==3,1,0)
long.data$dummy.gru <- ifelse(long.data$party==4,1,0)
long.data$dummy.fdp <- ifelse(long.data$party==5,1,0)
long.data$dummy.afd <- ifelse(long.data$party==6,1,0)
long.data$dummy.lin <- ifelse(long.data$party==7,1,0)

# check the dummy variable

head(long.data,n=10)

```

Now we can check whether evaluation of CDU is different from that of the other parties:

```{r}
summary(lm.out.cdu <- lm(skalo.true ~ lr.dist.abs + dummy.cdu,data=long.data))

```

The results can be visualized as follows:

```{r}

plot(long.data$skalo.true ~ long.data$lr.dist.abs,
     xlab="Close <- -> Distant",ylab="Dislike <-  -> Like",
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
abline(lm.out.abs,     lty=2,lwd=1)

predicted.cdu.0 <- coef(lm.out.cdu)[1] + coef(lm.out.cdu)[2]*c(0:10)
predicted.cdu.1 <- coef(lm.out.cdu)[1] + coef(lm.out.cdu)[2]*c(0:10) + coef(lm.out.cdu)[3]

par(new=T)
plot(predicted.cdu.0 ~ c(0:10),col="grey",type="l",
     ann=F,xlab="",ylab="",axes=F,
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.cdu.1 ~ c(0:10),col="orange",type="l",
     ann=F,xlab="",ylab="",axes=F,
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
legend("topright",col=c("black","orange","gray"),
       lty=c(2,1,1),c("All parties","CDU","Other parties"),
       bty="n")


```

We can find the intercept for CDU is slightly lower than the other parties. Its difference is however so small that it can be due to by chance.

We can now extend the model with the intercept for CSU:


```{r}

summary(lm.out.cdu.csu <- lm(skalo.true ~ lr.dist.abs + 
           dummy.cdu + dummy.csu ,
           data=long.data))

```

```{r}

plot(long.data$skalo.true ~ long.data$lr.dist.abs,
     xlab="Close <- -> Distant",ylab="Dislike <-  -> Like",
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
abline(lm.out.abs,     lty=2,lwd=1)

predicted.others <- coef(lm.out.cdu.csu)[1] + coef(lm.out.cdu.csu)[2]*c(0:10)
predicted.cdu <- coef(lm.out.cdu.csu)[1] + coef(lm.out.cdu.csu)[2]*c(0:10) + coef(lm.out.cdu.csu)[3]
predicted.csu <- coef(lm.out.cdu.csu)[1] + coef(lm.out.cdu.csu)[2]*c(0:10) + coef(lm.out.cdu.csu)[4]

par(new=T)
plot(predicted.others ~ c(0:10),col="grey",type="l",
     ann=F,xlab="",ylab="",axes=F,
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.cdu ~ c(0:10),col="orange",type="l",
     ann=F,xlab="",ylab="",axes=F,
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.csu ~ c(0:10),col="cyan",type="l",
     ann=F,xlab="",ylab="",axes=F,
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))

legend("topright",col=c("black","orange","cyan","gray"),
       lty=c(2,1,1,1),c("All parties","CDU","CSU","Other parties"),
       bty="n")

```

Here, the CSU's intercept is lower than the other parties and it is significant at 5% level.

To obtain the intercept for all parties, you regress the party evaluation on the left-right distance and all party dummy variables:


```{r}

summary(lm.out.all <- lm(skalo.true ~ lr.dist.abs + 
           dummy.cdu + dummy.csu + dummy.spd +
             dummy.gru + dummy.fdp + dummy.afd + dummy.lin,
           data=long.data))

```

The effect of the dummy variable for the Linke was not estimated. This is due to the perfect collinearity (the dummy variable trap). What is happening here can be seen in the following plot:



```{r}

plot(long.data$skalo.true ~ long.data$lr.dist.abs,
     xlab="Close <- -> Distant",ylab="Dislike <-  -> Like",
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
abline(lm.out.abs,     lty=2,lwd=2)

predicted.others <- coef(lm.out.all)[1] + coef(lm.out.all)[2]*c(0:10)
predicted.cdu <- coef(lm.out.all)[1] + coef(lm.out.all)[2]*c(0:10) + coef(lm.out.all)[3]
predicted.csu <- coef(lm.out.all)[1] + coef(lm.out.all)[2]*c(0:10) + coef(lm.out.all)[4]
predicted.spd <- coef(lm.out.all)[1] + coef(lm.out.all)[2]*c(0:10) + coef(lm.out.all)[5]
predicted.gru <- coef(lm.out.all)[1] + coef(lm.out.all)[2]*c(0:10) + coef(lm.out.all)[6]
predicted.fdp <- coef(lm.out.all)[1] + coef(lm.out.all)[2]*c(0:10) + coef(lm.out.all)[7]
predicted.afd <- coef(lm.out.all)[1] + coef(lm.out.all)[2]*c(0:10) + coef(lm.out.all)[8]
predicted.lin <- coef(lm.out.all)[1] + coef(lm.out.all)[2]*c(0:10) + coef(lm.out.all)[9]

par(new=T)
plot(predicted.others ~ c(0:10),col="grey",type="l",
     ann=F,xlab="",ylab="",axes=F,
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.cdu ~ c(0:10),col="orange",type="l",
     ann=F,xlab="",ylab="",axes=F,
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.csu ~ c(0:10),col="cyan",type="l",
     ann=F,xlab="",ylab="",axes=F,
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.spd ~ c(0:10),col="red",type="l",
     ann=F,xlab="",ylab="",axes=F,
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.gru ~ c(0:10),col="green",type="l",
     ann=F,xlab="",ylab="",axes=F,
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.fdp ~ c(0:10),col="yellow",type="l",
     ann=F,xlab="",ylab="",axes=F,
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.afd ~ c(0:10),col="blue4",type="l",
     ann=F,xlab="",ylab="",axes=F,
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))
par(new=T)
plot(predicted.lin ~ c(0:10),col="pink",type="l",
     ann=F,xlab="",ylab="",axes=F,
     xlim=range(long.data$lr.dist.abs,na.rm=T),
     ylim=range(long.data$skalo.true,na.rm=T))

legend("topright",col=c("black","orange","cyan","red","green","yellow","blue4","pink","gray"),
       lty=c(2,rep(1,8)),c("All parties","CDU","CSU","SPD","Greens","FDP","AfD","Linke",
                        "Other parties"),
       lwd=c(2,rep(1,8)),
       bty="n")

```

In the figure, you can see the regression lines for all parties except for the Linke. However, you see the gray line for the others. And this corresponds to the regression line for the linke. 

The equivalent model can be also estimated as follows:


```{r}
summary(lm.out.all2 <- lm(skalo.true ~ lr.dist.abs + as.factor(party),data=long.data))

```

In this model, the effect of the CDU-dummy was dropped. But the result is completely equivalent to the last model.


Now, we can observe the residuals to check the zero conditional mean:

```{r}
par(mfrow=c(1,2))
resid.1 <- residuals(lm.out.abs)
plot(resid.1 ~ long.data$lr.dist.abs,
     ylim=range(resid.1),xlim=c(0,10),
     ylab="Residuals",xlab="Distance",
     main="Model without party dummies")
E.u <- tapply(resid.1,as.factor(long.data$lr.dist.abs),mean)
par(new=T)
plot(E.u ~ c(0:10),
     ylim=range(resid.1),xlim=c(0,10),
     type="l",col="red",ann=F,xlab="",ylab="",axes=F)
abline(h=0,lty=2)

resid.5 <- residuals(lm.out.all2)
plot(resid.5 ~ long.data$lr.dist.abs,
     ylab="Residuals",xlab="Distance",
     main="Model with party dummies")
E.u <- tapply(resid.5,as.factor(long.data$lr.dist.abs),mean)
par(new=T)
plot(E.u ~ c(0:10),
     ylim=range(resid.5),xlim=c(0,10),
     type="l",col="red",ann=F,xlab="",ylab="",axes=F)
abline(h=0,lty=2)

```

Here, we still have a problem in the range for large ideological distance values.

## 7.4 Interactions involving dummy variables


Let's come back to the first model with the dummy variable for CDU evaluation:

```{r}
summary(lm.out.cdu)


```

While we estimated different intercepts for the evaluation of CDU and the other parties in the above model, we further assumed that the effect of the ideological distance is same for all parties. It is however not nescessarily true. The ideological distance can have different impacts on the evaluation of CDU and the other parties. To check this, we include now the interaction effect of both variables:

```{r}


summary(lm.out.cdu.int <- lm(skalo.true ~ lr.dist.abs * dummy.cdu,data=long.data))

```


This result is a bit tricky to interpret as you have already seen in a previous section. In the previous section, we visualized the results for ease of interpretation. This time, we conduct two separate regression analysis for CDU evaluation and the other evaluations. First we divide the dataset:

```{r}
cdu.dat <- long.data[long.data$party==1,]
non.cdu.dat <- long.data[long.data$party!=1,]

```

Subsequently, the simple regression model is estimated for the CDU evaluation...

```{r}
summary(lm.out.onlycdu <- lm(skalo.true ~ lr.dist.abs,data = cdu.dat))

```

... and the others.

```{r}
summary(lm.out.onlynoncdu <- lm( skalo.true ~ lr.dist.abs,data = non.cdu.dat))

```

The point estimates of both separate regression can be found in the first model, while the other elements (standard errors, degrees of freedom) are not always identical.


Now, we wish to test whether both regression models systematically differ from each other. The null-hypothesis is that both models are based on the same population model, which corresponds to the first model estimated without any interaction terms. We call the latter model the pooled model since it pools both groups into one dataset.

Here, the separated models constitute together an unrestricted model while the pooled model is restricted since we restrict the effects of both independent variables being same for both groups.

This can be tested by F-statistic (which is called "Chow statistic"):

$F = \frac{SSR_P - (SSR_1 + SSR_2)/(k+1)}{(SSR_1 + SSR_2)/[n-2(k+1)]}$

```{r}
SSR.p <- sum(lm.out.abs$residuals^2)
SSR.1 <- sum(lm.out.onlycdu$residuals^2)
SSR.2 <- sum(lm.out.onlynoncdu$residuals^2)
k <- 2

sum.df <- lm.out.onlycdu$df.residual+lm.out.onlynoncdu$df.residual


F <- ((SSR.p - (SSR.1+SSR.2))/(k+1))/((SSR.1+SSR.2)/(sum.df))
F

```

```{r, echo = FALSE}

x.scale <- seq(0,10,by=0.01)
this.y <- df(x.scale,df1=k+1,df2=sum.df)

plot(x.scale,this.y,type="l",xlab="t",ylab="Density",
       main=paste("F-distribution with df=",k+1,"and",sum.df))

this.95 <- qf(c(.95),df1=k+1,df2=sum.df)
abline(v=this.95,lty=2)
text(1,0.2,"95%")
text(this.95,0,pos=2,round(this.95,3))
text(this.95,0.2,"Rejection region",pos=4)

``` 

We can not reject here the null-hypothesis. 




```{r}
knitr::knit_exit()
```





```{r}

this.dat <- long.data[long.data$party==1,]

time.ten <- floor(this.dat$time/20)

p.est <- tapply(this.dat$skalo,as.factor(time.ten),mean)

this.func <- function(x) {
  out <- ci.sample.mean(x)
  out <- c(out$lower.b,out$upper.b)
  }

int.est <- tapply(this.dat$skalo,as.factor(time.ten),this.func)

int.est

plot(p.est ~ as.numeric(names(p.est)))


table(time.ten)

```

