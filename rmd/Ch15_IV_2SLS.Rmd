---
title: 'Chapter 15: Instrumental Variables Estimation and 2 Stage Least Squares'
author: "Susumu Shikano"
date: "Last compiled at `r format(Sys.Date(), '%d. %B %Y')`"
output: html_document
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# loading some functions
source("https://raw.githubusercontent.com/SuShikano/StatTeaching/main/func/data_generation.R")
source("https://raw.githubusercontent.com/SuShikano/StatTeaching/main/func/ci_sample_mean.R")
source("https://raw.githubusercontent.com/SuShikano/StatTeaching/main/func/cond_mean_plot.R")
source("https://raw.githubusercontent.com/SuShikano/StatTeaching/main/func/naive_var.R")
source("https://raw.githubusercontent.com/SuShikano/StatTeaching/main/func/naive_cov.R")
source("https://raw.githubusercontent.com/SuShikano/StatTeaching/main/func/robust_se_srm.R")

# setting some parameters

num.datasets <- 1000 # number of datasets
sample.size <- 1000

```

## Instrumental variable estimation

```{r}
n.iv <- 2
true.slope <- c(0.4, -0.3)
true.intercept <- 0.4

x.mu <- c(0.6,5)
x.Sigma <- cbind(c(3,-1),
                 c(-1,2))
true.err.var <- 0.05

```


We generate `r num.datasets` datasets with n=`r sample.size` under the GM-assumptions. The number of independent variables is `r n.iv`. The true regression line has the intercept of `r true.intercept` and the slope of `r true.slope`. The independent variables are generated with the mean `r x.mu`, variances  `r diag(x.Sigma)`.



```{r}

samples.iv <- data.generation(sample.size=sample.size,
                             n.sim=num.datasets,
                             n.iv=n.iv,
                             x.mu=x.mu,
                             x.Sigma=x.Sigma,
                             para=c(true.intercept,true.slope),
                             err.dist = "normal",
                             err.disp = true.err.var,
                             het= FALSE,
                             binary.y = FALSE,
                             instv = TRUE,
                             instv.num = 1,
                             instv.mu.cov = c(2,2.5,2,0))

```


We can estimate the following two regression models $$\hat y = \tilde \beta_0 + \tilde \beta_1 x_1$$ and $$\hat y = \hat \beta_0 + \hat \beta_1 x_1 + \hat \beta_2 x_2$$ and compare both slope estimates.


```{r}

all.coef <- array(NA,dim=c(num.datasets,4,7))
all.coef.se <- array(NA,dim=c(num.datasets,3,7))

for (i in 1:num.datasets){
    this.data <- samples.iv$generated.data[[i]]

    lm.out <- lm(y ~ X1 + X2 ,data=  this.data)

    all.coef[i,1:3,1] <- coef(lm.out)
    all.coef[i,4,1] <- summary(lm.out)$sigma
    all.coef.se[i,,1] <- coef(summary(lm.out))[,2]
    
    lm.out <- lm(y ~ X1  ,data=  this.data)

    all.coef[i,1:2,2] <- coef(lm.out)
    all.coef[i,4,2] <- summary(lm.out)$sigma
    all.coef.se[i,c(1:2),2] <- coef(summary(lm.out))[,2]

    # iv estimator
    all.coef[i,2,3] <- cov(this.data$IV,this.data$y)/cov(this.data$IV,this.data$X1)
    all.coef[i,1,3] <- mean(this.data$y) - all.coef[i,2,3]*mean(this.data$X1)

    resid <- this.data$y - (all.coef[i,1,3] + all.coef[i,2,3]*this.data$X1)
        
    sigma2.hat <- sum(resid^2)/(nrow(this.data)-2)
    SSTx <- sum((this.data$X1 - mean(this.data$X1))^2)
    R2.xz <- summary(lm(this.data$X1 ~ this.data$IV))$r.squared
    
    all.coef.se[i,2,3] <- sigma2.hat/(SSTx*R2.xz)

    }
dimnames(all.coef)[[2]] <- c("b0","b1","b2","sigma")
dimnames(all.coef.se)[[2]] <- c("b0","b1","b2")


```


Below you will find the distribution of both estimated regression coefficients:


```{r}

    x.range <- range(c(all.coef[,"b1",]),na.rm=T)
    density.out <- density(all.coef[,"b1",1])
    plot(density.out,
         main="",xlab=paste0("Regression coefficients for X1"),
         xlim=x.range,
         ylim=c(0,max(density.out$y)))
    abline(v=mean(all.coef[,"b1",1]),lty=1)
    par(new=T)
    plot(density(all.coef[,"b1",2]),ann=F,xlab="",ylab="",
         axes=F,
         xlim=x.range,lty=2,
         ylim=c(0,max(density.out$y)))
    abline(v=mean(all.coef[,"b1",2]),lty=2)
    legend("topright",lty=c(1,2),c("X1 + X2","Only X1"),bty="n")
    

```

Obviously, the mis-specified model with only X1 leads to strongly biased estimates. 

Now, we can keep the mis-specified model with only X1 and adopt the instrumental variable estimator. An instrumental variable $z$ fulfills in the model $$y = \beta_0 + \beta_1 x_1 + u$$ the following two conditions: $Cov(z,u)=0$ and $Cov(x_1,z) \neq 0$.


If another model $$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + e$$ fulfills the GM-assumptions, an instrumental variable can be generated as random number whose covariance with X2 is zero and with X1 nonzero. 


We can check using one of the generated sample, whether our instrumental variable fulfills the conditions:

```{r}
data1 <- samples.iv$generated.data[[1]]

print(cor(data1),digits=3)

```

It has be here noted that the "error" in this table does not corresponds to $u$, but $e$ in the above equations. The $u$ above consists of $e$ and $\beta_2 x_2$. Therefore, the instrumental variable has to be generated with covariance with both $e$ and $x_2$ being zero.



```{r}

    x.range <- range(c(all.coef[,"b1",]),na.rm=T)
    density.out <- density(all.coef[,"b1",1])
    plot(density.out,
         main="",xlab=paste0("Regression coefficients for X1"),
         xlim=x.range,
         ylim=c(0,max(density.out$y)))
    abline(v=mean(all.coef[,"b1",1]),lty=1)
    par(new=T)
    plot(density(all.coef[,"b1",2]),ann=F,xlab="",ylab="",
         axes=F,
         xlim=x.range,lty=2,
         ylim=c(0,max(density.out$y)))
    abline(v=mean(all.coef[,"b1",2]),lty=2)
    par(new=T)
    plot(density(all.coef[,"b1",3]),ann=F,xlab="",ylab="",
         axes=F,
         xlim=x.range,lty=4,lwd=2,
         ylim=c(0,max(density.out$y)))
    abline(v=mean(all.coef[,"b1",3]),lty=4,lwd=2)
    legend("topright",lty=c(1,2,4),lwd=c(1,1,2),
           c("X1 + X2","Only X1","X1 + IV"),bty="n")
    

```

It is clearly to see that the instrumental variable estimator provides unbiased results, but with larger uncertainty. That is, the OLS estimator based on the correctly specified model is better than the IV estimator (see also the Gauss-Markov Theorem and "Asymptotic Efficiency of the OLS estimator" in Chapter 5). 


## Poor instrumental variables

```{r}
cov.z.x2 <- 0.2
weak.cov.z.x <- 1
```


What will happen if we use a poor instrumental variable? An instrumental variable can be poor in the following two senses:

* IV is correlated with u.
* IV is only weakly correlated with $x_1$.

To see the consequences, we generate two different sets of samples. In the first set, the covariance of the instrumental variable and $x_2$ is `r cov.z.x2`, which means the covariance of the instrumental variable and $u$ is $\beta_2 \times$ `r cov.z.x2`. In the second set, we further reduce the covariance of the instrumental variable and $x_1$ to `r weak.cov.z.x`.


```{r}

samples.iv.poor1 <- data.generation(sample.size=sample.size,
                             n.sim=num.datasets,
                             n.iv=n.iv,
                             x.mu=x.mu,
                             x.Sigma=x.Sigma,
                             para=c(true.intercept,true.slope),
                             err.dist = "normal",
                             err.disp = true.err.var,
                             het= FALSE,
                             binary.y = FALSE,
                             instv = TRUE,
                             instv.num = 1,
                             instv.mu.cov = c(2,2.5,2,cov.z.x2))

samples.iv.poor2 <- data.generation(sample.size=sample.size,
                             n.sim=num.datasets,
                             n.iv=n.iv,
                             x.mu=x.mu,
                             x.Sigma=x.Sigma,
                             para=c(true.intercept,true.slope),
                             err.dist = "normal",
                             err.disp = true.err.var,
                             het= FALSE,
                             binary.y = FALSE,
                             instv = TRUE,
                             instv.num = 1,
                             instv.mu.cov = c(2,2.5,weak.cov.z.x,cov.z.x2))


```

By using both sets of samples, we estimate the effect of $x_1$:


```{r}


for (i in 1:num.datasets){

    # iv estimator
    this.data <- samples.iv.poor1$generated.data[[i]]

    all.coef[i,2,4] <- cov(this.data$IV,this.data$y)/cov(this.data$IV,this.data$X1)
    all.coef[i,1,4] <- mean(this.data$y) - all.coef[i,2,4]*mean(this.data$X1)

    resid <- this.data$y - (all.coef[i,1,4] + all.coef[i,2,4]*this.data$X1)
        
    sigma2.hat <- sum(resid^2)/(nrow(this.data)-2)
    SSTx <- sum((this.data$X1 - mean(this.data$X1))^2)
    R2.xz <- summary(lm(this.data$X1 ~ this.data$IV))$r.squared
    
    all.coef.se[i,2,4] <- sigma2.hat/(SSTx*R2.xz)

    # iv estimator
    this.data <- samples.iv.poor2$generated.data[[i]]

    all.coef[i,2,5] <- cov(this.data$IV,this.data$y)/cov(this.data$IV,this.data$X1)
    all.coef[i,1,5] <- mean(this.data$y) - all.coef[i,2,5]*mean(this.data$X1)

    resid <- this.data$y - (all.coef[i,1,5] + all.coef[i,2,5]*this.data$X1)
        
    sigma2.hat <- sum(resid^2)/(nrow(this.data)-2)
    SSTx <- sum((this.data$X1 - mean(this.data$X1))^2)
    R2.xz <- summary(lm(this.data$X1 ~ this.data$IV))$r.squared
    
    all.coef.se[i,2,5] <- sigma2.hat/(SSTx*R2.xz)
    
    
    }


```








```{r}

    x.range <- range(c(all.coef[,"b1",]),na.rm=T)
    density.out <- density(all.coef[,"b1",1])
    plot(density.out,
         main="",xlab=paste0("Regression coefficients for X1"),
         xlim=x.range,
         ylim=c(0,max(density.out$y)))
    abline(v=mean(all.coef[,"b1",1]),lty=1)
    par(new=T)
    plot(density(all.coef[,"b1",2]),ann=F,xlab="",ylab="",
         axes=F,
         xlim=x.range,lty=2,
         ylim=c(0,max(density.out$y)))
    abline(v=mean(all.coef[,"b1",2]),lty=2)
    par(new=T)
    plot(density(all.coef[,"b1",3]),ann=F,xlab="",ylab="",
         axes=F,
         xlim=x.range,lty=4,lwd=1,
         ylim=c(0,max(density.out$y)))
    abline(v=mean(all.coef[,"b1",3]),lty=4,lwd=1)

    par(new=T)
    plot(density(all.coef[,"b1",4]),ann=F,xlab="",ylab="",
         axes=F,
         xlim=x.range,lty=5,lwd=2,
         ylim=c(0,max(density.out$y)))
    abline(v=mean(all.coef[,"b1",4]),lty=5,lwd=2)
    
    par(new=T)
    plot(density(all.coef[,"b1",5]),ann=F,xlab="",ylab="",
         axes=F,
         xlim=x.range,lty=6,lwd=2,
         ylim=c(0,max(density.out$y)))
    abline(v=mean(all.coef[,"b1",5]),lty=6,lwd=2)
    
    
    
    
    legend("topright",lty=c(1,2,4,5,6),lwd=c(1,1,1,2,2),
           c("X1+X2","Only X1","X1+IV","X1+poor IV","X1+poor IV"),bty="n")
    

```


Use of the instrumental variable with is correlated with the error leads to biased estimates, while the uncertainty remains stable. Given the IV being correlated with the error, if we reduce the covariance of the IV and the $x_1$, the bias is even larger and the uncertainty is also larger. These results corresponds to the probability limit of IV estimator: $$plim \hat \beta_{1,IV} = \beta_1 + \frac{Corr(z.u)}{Corr(z,x)} \frac{\sigma_u}{\sigma_x} $$ and the asymptotic standard error of $\hat \beta_1$: $$ SE(\hat \beta_1) = \frac{\hat \sigma^2}{SST_x R^2_{x,z}} $$.





## 2SLS: in case of multiple instrumental variables


For a certain endogenous explanatory variable, there can exist multiple instrumental variables. We generate a set of samples under such a circumstance. More specifically, we create an additional variable which is not correlated with $x_2$ and whose correlation with $x_1$ is slightly lower than that of the previous instrumental variable.


```{r}

samples.iv.multi <- data.generation(sample.size=sample.size,
                             n.sim=num.datasets,
                             n.iv=n.iv,
                             x.mu=x.mu,
                             x.Sigma=x.Sigma,
                             para=c(true.intercept,true.slope),
                             err.dist = "normal",
                             err.disp = true.err.var,
                             het= FALSE,
                             binary.y = FALSE,
                             instv = TRUE,
                             instv.num = 2,
                             instv.mu.cov = c(2,1,  # means
                                              2.5,1.5,  # vars
                                              2,1.5, # cov with x1
                                              0,0, # cov with x2
                                              0.5)) # cov of z1 and z2


```


Now, we can use the new instrumental variable to estimate the effect of $x_1$ ony $y$. Further, we can use 2SLS with both instrumental variables, simultaneously. 






```{r}


for (i in 1:num.datasets){

    # iv estimator
    this.data <- samples.iv.multi$generated.data[[i]]

    all.coef[i,2,6] <- cov(this.data$IV2,this.data$y)/cov(this.data$IV2,this.data$X1)
    all.coef[i,1,6] <- mean(this.data$y) - all.coef[i,2,6]*mean(this.data$X1)

    resid <- this.data$y - (all.coef[i,1,6] + all.coef[i,2,6]*this.data$X1)
        
    sigma2.hat <- sum(resid^2)/(nrow(this.data)-2)
    SSTx <- sum((this.data$X1 - mean(this.data$X1))^2)
    R2.xz <- summary(lm(this.data$X1 ~ this.data$IV2))$r.squared
    
    all.coef.se[i,2,6] <- sigma2.hat/(SSTx*R2.xz)
    
    # 2sls
    ls.1 <- lm(X1 ~ IV1 + IV2 , data=this.data)
    this.data$new.inst <- ls.1$fitted.values
    
    ls.2 <- lm(y ~ new.inst, data=this.data)

    all.coef[i,1:2,7] <- coef(ls.2)
    all.coef[i,4,7] <- summary(ls.2)$sigma
    all.coef.se[i,1:2,7] <- coef(summary(ls.2))[,2]

    }


```


The estimates are presented in the following figure:




```{r}

    x.range <- range(c(all.coef[,"b1",c(1,2,3,6,7)]),na.rm=T)
    density.out <- density(all.coef[,"b1",1])
    plot(density.out,
         main="",xlab=paste0("Regression coefficients for X1"),
         xlim=x.range,
         ylim=c(0,max(density.out$y)))
    abline(v=mean(all.coef[,"b1",1]),lty=1)
    par(new=T)
    plot(density(all.coef[,"b1",2]),ann=F,xlab="",ylab="",
         axes=F,
         xlim=x.range,lty=2,
         ylim=c(0,max(density.out$y)))
    abline(v=mean(all.coef[,"b1",2]),lty=2)
    par(new=T)
    plot(density(all.coef[,"b1",3]),ann=F,xlab="",ylab="",
         axes=F,
         xlim=x.range,lty=4,lwd=1,
         ylim=c(0,max(density.out$y)))
    abline(v=mean(all.coef[,"b1",3]),lty=4,lwd=1)

    par(new=T)
    plot(density(all.coef[,"b1",6]),ann=F,xlab="",ylab="",
         axes=F,
         xlim=x.range,lty=5,lwd=2,
         ylim=c(0,max(density.out$y)))
    abline(v=mean(all.coef[,"b1",6]),lty=5,lwd=2)
    
    par(new=T)
    plot(density(all.coef[,"b1",7]),ann=F,xlab="",ylab="",
         axes=F,
         xlim=x.range,lty=6,lwd=2,
         ylim=c(0,max(density.out$y)))
    abline(v=mean(all.coef[,"b1",7]),lty=6,lwd=2)
    
    
    
    
    legend("topright",lty=c(1,2,4,5,6),lwd=c(1,1,1,2,2),
           c("X1 + X2","Only X1","X1 + IV","another IV","2SLS"),bty="n")
    

```


If one compares the estimates based on the new instrumental variable, there is no clear difference from the other instrumental variable. If one use both in 2SLS, in contrast, the uncertainty of the estimates becomes smaller. 



