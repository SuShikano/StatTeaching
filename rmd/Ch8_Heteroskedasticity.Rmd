---
title: 'Chapter 8: Heteroskedasticity'
author: "Susumu Shikano"
date: "Last compiled at `r format(Sys.Date(), '%d. %B %Y')`"
output: html_document
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# loading some functions
source("https://raw.githubusercontent.com/SuShikano/StatTeaching/main/func/data_generation.R")
source("https://raw.githubusercontent.com/SuShikano/StatTeaching/main/func/ci_sample_mean.R")
source("https://raw.githubusercontent.com/SuShikano/StatTeaching/main/func/cond_mean_plot.R")
source("https://raw.githubusercontent.com/SuShikano/StatTeaching/main/func/naive_var.R")
source("https://raw.githubusercontent.com/SuShikano/StatTeaching/main/func/naive_cov.R")
source("https://raw.githubusercontent.com/SuShikano/StatTeaching/main/func/robust_se_srm.R")

# setting some parameters

num.datasets <- 1000 # number of datasets
sample.size <- 500

```

## Simple regression model

```{r}
n.iv <- 1
true.slope <- 0.4
true.intercept <- 0.4

x.mu <- 0.6
x.Sigma <- as.matrix(0.5)

true.err.var <- 0.05

```


We generate `r num.datasets` datasets with n=`r sample.size` under the GM-assumptions. The number of independent variables is `r n.iv`. The true regression line has the intercept of `r true.intercept` and the slope of `r true.slope`. The independent variables are generated with the mean `r x.mu`, variances  `r diag(x.Sigma)`.



```{r}

samples.srm.gm <- data.generation(sample.size=sample.size,
                             n.sim=num.datasets,
                             n.iv=n.iv,
                             x.mu=x.mu,
                             x.Sigma=x.Sigma,
                             para=c(true.intercept,true.slope),
                             err.dist = "normal",
                             err.disp = true.err.var,
                             het= FALSE,
                             binary.y = FALSE)

het.delta <- c(0.5,1.3)
samples.srm.het <- data.generation(sample.size=sample.size,
                             n.sim=num.datasets,
                             n.iv=n.iv,
                             x.mu=x.mu,
                             x.Sigma=x.Sigma,
                             para=c(true.intercept,true.slope),
                             err.dist = "normal",
                             err.disp = true.err.var,
                             het= TRUE,
                             het.delta = het.delta,
                             binary.y = FALSE)


```


Another set of samples is generated with the same parameters above, but without the homoskedasticity assumption. Here, the error variance is assumed to be: $$var(u|x_1) = \sigma^2 \exp(\delta_0 + \delta_1 x_1)$$ with $$\delta_0= `r het.delta[1]`$$ and $$\delta_1= `r het.delta[2]`$$.


Below, we can see a sample data generated under different assumptions. The joint distribution in the right-hand side panel demonstrates an obvious heteroskedasticity.


```{r}
par(mfrow=c(1,2))

for (i.fig in 1:2){

  if (i.fig==1) data.1 <- samples.srm.gm$generated.data[[1]]
  if (i.fig==2) data.1 <- samples.srm.het$generated.data[[1]]

  plot(data.1$X1,data.1$y,
       xlab="X1",ylab="Y",
       main=c("GM","Heteroskedasticity")[i.fig])
  abline(reg=samples.srm.het$para,col="red")
  abline(lm(y ~ X1 ,data=data.1))
  legend("topleft",lty=1,col=c("red","black"),c("True","Estimate"),bty="n")
  
}

```


We can now estimate the regression model for each sample.

```{r}

all.coef <- array(NA,dim=c(num.datasets,3,2))
all.coef.se <- array(NA,dim=c(num.datasets,3,2))

for (i in 1:num.datasets){
    this.data <- samples.srm.gm$generated.data[[i]]

    lm.out <- lm(y ~ X1  ,data=  this.data)

    all.coef[i,1:2,1] <- coef(lm.out)
    all.coef[i,3,1] <- summary(lm.out)$sigma
    all.coef.se[i,c(1:2),1] <- coef(summary(lm.out))[,2]
    
    all.coef.se[i,3,1] <- robust.se.srm(lm.out)

    this.data <- samples.srm.het$generated.data[[i]]

    lm.out <- lm(y ~ X1  ,data=  this.data)

    all.coef[i,1:2,2] <- coef(lm.out)
    all.coef[i,3,2] <- summary(lm.out)$sigma
    all.coef.se[i,c(1:2),2] <- coef(summary(lm.out))[,2]
    
    all.coef.se[i,3,2] <- robust.se.srm(lm.out)
    }
dimnames(all.coef)[[2]] <- c("b0","b1","sigma")
dimnames(all.coef.se)[[2]] <- c("b0","b1","robust")


```


Below, you can see the distribution of the point estimates:


```{r}

    x.range <- range(c(all.coef[,"b1",]))
    plot(density.out <- density(all.coef[,"b1",1]),
         main="",xlab=paste0("Estimates"),
         xlim=x.range)
    abline(v=mean(all.coef[,"b1",1]))
    par(new=T)
    plot(density(all.coef[,"b1",2]),ann=F,xlab="",ylab="",main="",
         axes=F,
         xlim=x.range,lty=2,
         ylim=c(0,max(density.out$y)))
    
    sds <- c(sqrt(naive.var(all.coef[,"b1",1])),
             sqrt(naive.var(all.coef[,"b1",2])))
    abline(v=mean(all.coef[,"b1",2]),lty=2)
    legend("topright",lty=c(1,NA,2,NA),
           c("Under GM",
           paste("SD:",round(sds[1],3)),
           "Heteroskedasticity",
           paste("SD:",round(sds[2],3))),bty="n")
    

```


If one looks at the expected values, both distributions have their expected value very close to the true value (`r true.slope`). That is, the OLS estimator is unbiased independently whether the data was generated with or without homoskedasticity. 

Both distribution differ in their dispersion. This is reasonable since the data under the heteroskedasticity was generated with a larger error variance $$u^2 = \sigma^2 exp(\delta_0 + \delta_1 X1 +... \delta_J X_j)$$.


Important here is whether the standard errors obtained based on samples corresponds to the dispersion above. Below, you can see the distribution of standard errors: 




```{r}

    x.range <- range(c(all.coef.se[,"b1",]))
    plot(density.out <- density(all.coef.se[,"b1",1]),
         main="",xlab=paste0("Standard errors"),
         xlim=x.range)
    #abline(v=mean(all.coef.se[,"b1",1]))
    par(new=T)
    plot(density(all.coef.se[,"b1",2]),ann=F,xlab="",ylab="",main="",
         axes=F,
         xlim=x.range,lty=2,
         ylim=c(0,max(density.out$y)))
    means <- c(mean(all.coef.se[,"b1",1]),
               mean(all.coef.se[,"b1",2]))
    #abline(v=mean(all.coef[,"b1",2]),lty=2)
    legend("topright",lty=c(1,NA,2,NA),
           c("Under GM",
           paste("Mean:",round(means[1],3)),
           "Heteroskedasticity",
           paste("Mean:",round(means[2],3))),bty="n")
    

```


Obviously, the mean standard error underestimate the dispersion of the point estimates above.



How about the robsut standard errors? Below, you will find the distributions.



```{r}

    x.range <- range(c(all.coef.se[,"robust",]))
    plot(density.out <- density(all.coef.se[,"robust",1]),
         main="",xlab=paste0("Robust standard errors"),
         xlim=x.range)
    #abline(v=mean(all.coef.se[,"b1",1]))
    par(new=T)
    plot(density(all.coef.se[,"robust",2]),ann=F,xlab="",ylab="",main="",
         axes=F,
         xlim=x.range,lty=2,
         ylim=c(0,max(density.out$y)))
    means <- c(mean(all.coef.se[,"robust",1]),
               mean(all.coef.se[,"robust",2]))
    #abline(v=mean(all.coef[,"b1",2]),lty=2)
    legend("topright",lty=c(1,NA,2,NA),
           c("Under GM",
           paste("Mean:",round(means[1],3)),
           "Heteroskedasticity",
           paste("Mean:",round(means[2],3))),bty="n")
    

```

Here, the mean of both standard errors is close to the dispersion of the point estimates above.


We can compare both types of standard errors:


```{r}

par(mfrow=c(1,2))

for (i.fig in 1:2){

plot(all.coef.se[,"b1",i.fig],all.coef.se[,"robust",i.fig],
     ylab="Robust SE",xlab="Naive SE",
     main=c("GM","Heteroskedasticity")[i.fig])
abline(coef=c(0,1),lty=2)
  
}



```



## Multiple regression model


```{r}
n.iv <- 2
true.slope <- c(0.4, -0.3)
true.intercept <- 0.4

x.mu <- c(0.6,5)
x.Sigma <- cbind(c(3,-1),
                 c(-1,2))
true.err.var <- 0.05

```


We generate `r num.datasets` datasets with the same parameters above, except:

* The number of independent variables is `r n.iv`. 
* True slope values are `r true.slope`. 
* The independent variables are generated with the mean `r x.mu`, and variances  `r diag(x.Sigma)` and covariance `r x.Sigma[1,2]`.



```{r}

samples.srm.gm <- data.generation(sample.size=sample.size,
                             n.sim=num.datasets,
                             n.iv=n.iv,
                             x.mu=x.mu,
                             x.Sigma=x.Sigma,
                             para=c(true.intercept,true.slope),
                             err.dist = "normal",
                             err.disp = true.err.var,
                             het= FALSE,
                             binary.y = FALSE)

het.delta <- c(0.5,1.3,0.5)
samples.srm.het <- data.generation(sample.size=sample.size,
                             n.sim=num.datasets,
                             n.iv=n.iv,
                             x.mu=x.mu,
                             x.Sigma=x.Sigma,
                             para=c(true.intercept,true.slope),
                             err.dist = "normal",
                             err.disp = true.err.var,
                             het= TRUE,
                             het.delta = het.delta,
                             binary.y = FALSE)


```


Another set of samples is generated with the same parameters above, but without the homoskedasticity assumption. Here, the error variance is assumed to be: $$var(u|x_1) = \sigma^2 \exp(\delta_0 + \delta_1 x_1+ \delta_2 x_2)$$ with $$\delta_0=`r het.delta[1]`$$ and $$\delta_1=`r het.delta[2]`$$ and $$\delta_2=`r het.delta[3]` $$.




We can now estimate the regression model for each sample.

```{r}

all.coef <- array(NA,dim=c(num.datasets,4,2))
all.coef.se <- array(NA,dim=c(num.datasets,5,2))

for (i in 1:num.datasets){
    this.data <- samples.srm.gm$generated.data[[i]]

    lm.out <- lm(y ~ X1 +X2 ,data=  this.data)

    all.coef[i,1:3,1] <- coef(lm.out)
    all.coef[i,4,1] <- summary(lm.out)$sigma
    all.coef.se[i,c(1:3),1] <- coef(summary(lm.out))[,2]
    
    #all.coef.se[i,3,1] <- robust.se.srm(lm.out)

    this.data <- samples.srm.het$generated.data[[i]]

    lm.out <- lm(y ~ X1 +X2 ,data=  this.data)

    all.coef[i,1:3,2] <- coef(lm.out)
    all.coef[i,4,2] <- summary(lm.out)$sigma
    all.coef.se[i,c(1:3),2] <- coef(summary(lm.out))[,2]
    
    #all.coef.se[i,3,2] <- robust.se.srm(lm.out)
    }
dimnames(all.coef)[[2]] <- c("b0","b1","b2","sigma")
dimnames(all.coef.se)[[2]] <- c("b0","b1","b2","b1robust","b2robust")


```


Below, you can see the distribution of the point estimates:


```{r}

par(mfrow=c(1,2))
for (i.fig in 1:2){

  target <- paste0("b",i.fig)

    x.range <- range(c(all.coef[,target,]))
    plot(density.out <- density(all.coef[,target,1]),
         main=target,xlab=paste0("Estimates"),
         xlim=x.range)
    abline(v=mean(all.coef[,target,1]))
    par(new=T)
    plot(density(all.coef[,target,2]),ann=F,xlab="",ylab="",main="",
         axes=F,
         xlim=x.range,lty=2,
         ylim=c(0,max(density.out$y)))
    
    sds <- c(sqrt(naive.var(all.coef[,target,1])),
             sqrt(naive.var(all.coef[,target,2])))
    abline(v=mean(all.coef[,target,2]),lty=2)
    legend("topright",lty=c(1,NA,2,NA),
           c("Under GM",
           paste("SD:",round(sds[1],3)),
           "Heteroskedasticity",
           paste("SD:",round(sds[2],3))),bty="n")
  
}
    

```


If one looks at the expected values, both distributions have their expected value very close to the true value (`r true.slope`). That is, the OLS estimator is unbiased independently whether the data was generated with or without homoskedasticity. 

Both distribution differ in their dispersion. This is reasonable since the data under the heteroskedasticity was generated with a larger error variance $$u^2 = \sigma^2 exp(\delta_0 + \delta_1 X1 +... \delta_J X_j)$$.


Important here is whether the standard errors obtained based on samples corresponds to the dispersion above. Below, you can see the distribution of standard errors: 




```{r}

par(mfrow=c(1,2))
for (i.fig in 1:2){

  target <- paste0("b",i.fig)

    x.range <- range(c(all.coef.se[,target,]))
    plot(density.out <- density(all.coef.se[,target,1]),
         main=target,xlab=paste0("Standard errors"),
         xlim=x.range)
    #abline(v=mean(all.coef.se[,target,1]))
    par(new=T)
    plot(density(all.coef.se[,target,2]),ann=F,xlab="",ylab="",main="",
         axes=F,
         xlim=x.range,lty=2,
         ylim=c(0,max(density.out$y)))
    means <- c(mean(all.coef.se[,target,1]),
               mean(all.coef.se[,target,2]))
    #abline(v=mean(all.coef[,target,2]),lty=2)
    legend("topright",lty=c(1,NA,2,NA),
           c("Under GM",
           paste("Mean:",round(means[1],3)),
           "Heteroskedasticity",
           paste("Mean:",round(means[2],3))),bty="n")
    
}

```


Obviously, the mean standard error underestimate the dispersion of the point estimates above.


### Weighted least squres (WLS)

Instead of using the robust standard errors, we can compute the WLS estimates. As weight, we use the second independent variable. 



```{r, warning=FALSE}

wls.coef <- array(NA,dim=c(num.datasets,4,2))
wls.coef.se <- array(NA,dim=c(num.datasets,3,2))

for (i in 1:num.datasets){
    this.data <- samples.srm.gm$generated.data[[i]]
    
    this.data <- cbind(this.data,1)
    colnames(this.data)[ncol(this.data)] <- "X0"

    root.h <- matrix(rep(this.data$X2,ncol(this.data)),ncol=ncol(this.data))    
    root.h <- sqrt(root.h)
    
    this.data <- this.data/root.h

    lm.out <- lm(y ~ 0 + X0 + X1 +X2 ,data=  this.data)

    wls.coef[i,1:3,1] <- coef(lm.out)
    wls.coef[i,4,1] <- summary(lm.out)$sigma
    wls.coef.se[i,c(1:3),1] <- coef(summary(lm.out))[,2]
    
    this.data <- samples.srm.het$generated.data[[i]]

    this.data <- cbind(this.data,1)
    colnames(this.data)[ncol(this.data)] <- "X0"

    root.h <- matrix(rep(this.data$X2,ncol(this.data)),ncol=ncol(this.data))    
    root.h <- sqrt(root.h)
    
    this.data <- this.data/root.h

    lm.out <- lm(y ~ 0 + X0 + X1 +X2 ,data=  this.data)

    wls.coef[i,1:3,2] <- coef(lm.out)
    wls.coef[i,4,2] <- summary(lm.out)$sigma
    wls.coef.se[i,c(1:3),2] <- coef(summary(lm.out))[,2]
    
    }
dimnames(wls.coef)[[2]] <- c("b0","b1","b2","sigma")
dimnames(wls.coef.se)[[2]] <- c("b0","b1","b2")


```




Below, you can see the distribution of the point estimates and standard errors:


```{r}

par(mfrow=c(1,2))
for (i.fig in 1:2){

  target <- paste0("b",i.fig)

    x.range <- range(c(wls.coef[,target,]))
    plot(density.out <- density(all.coef[,target,1]),
         main=target,xlab=paste0("Estimates"),
         xlim=x.range)
    abline(v=mean(wls.coef[,target,1]))
    par(new=T)
    plot(density(wls.coef[,target,2]),ann=F,xlab="",ylab="",main="",
         axes=F,
         xlim=x.range,lty=2,
         ylim=c(0,max(density.out$y)))
    
    sds <- c(sqrt(naive.var(wls.coef[,target,1])),
             sqrt(naive.var(wls.coef[,target,2])))
    abline(v=mean(wls.coef[,target,2]),lty=2)
    legend("topright",lty=c(1,NA,2,NA),
           c("Under GM",
           paste("SD:",round(sds[1],3)),
           "Heteroskedasticity",
           paste("SD:",round(sds[2],3))),bty="n")
  
}
    


par(mfrow=c(1,2))
for (i.fig in 1:2){

  target <- paste0("b",i.fig)

    x.range <- range(c(wls.coef.se[,target,]))
    plot(density.out <- density(wls.coef.se[,target,1]),
         main=target,xlab=paste0("Standard errors"),
         xlim=x.range)
    #abline(v=mean(all.coef.se[,target,1]))
    par(new=T)
    plot(density(wls.coef.se[,target,2]),ann=F,xlab="",ylab="",main="",
         axes=F,
         xlim=x.range,lty=2,
         ylim=c(0,max(density.out$y)))
    means <- c(mean(wls.coef.se[,target,1]),
               mean(wls.coef.se[,target,2]))
    #abline(v=mean(all.coef[,target,2]),lty=2)
    legend("topright",lty=c(1,NA,2,NA),
           c("Under GM",
           paste("Mean:",round(means[1],3)),
           "Heteroskedasticity",
           paste("Mean:",round(means[2],3))),bty="n")
    
}

```


This performs quite poor because X2 contributes only small part of the heteroskedasticity in the data generating process.


### Feasible generalized least squares (FGLS)

Instead of using a certain variable, we can now try to estimate the weight by using the OLS results.



```{r, warning=FALSE}

fgls.coef <- array(NA,dim=c(num.datasets,4,2))
fgls.coef.se <- array(NA,dim=c(num.datasets,3,2))
fgls.h.hat <- array(NA,dim=c(num.datasets,sample.size,2))

for (i in 1:num.datasets){
    this.data <- samples.srm.gm$generated.data[[i]]
    
    ols.out <- lm(y ~ X1 +X2 ,data=  this.data)
    
    this.data.2 <- this.data
    this.data.2[,1] <- log(ols.out$residuals^2)
    
    ols.out.2 <- lm(y ~ X1 +X2 ,data=  this.data.2)
    g.hat <- ols.out.2$fitted.values
    h.hat <- exp(g.hat)

    this.data <- cbind(this.data,1)
    colnames(this.data)[ncol(this.data)] <- "X0"

    root.h <- matrix(rep(h.hat,ncol(this.data)),ncol=ncol(this.data))    
    root.h <- sqrt(root.h)
    
    this.data <- this.data/root.h

    lm.out <- lm(y ~ 0 + X0 + X1 +X2 ,data=  this.data)

    fgls.coef[i,1:3,1] <- coef(lm.out)
    fgls.coef[i,4,1] <- summary(lm.out)$sigma
    fgls.coef.se[i,c(1:3),1] <- coef(summary(lm.out))[,2]
    fgls.h.hat[i,,1] <- h.hat
    
    this.data <- samples.srm.het$generated.data[[i]]

    ols.out <- lm(y ~ X1 +X2 ,data=  this.data)
    
    this.data.2 <- this.data
    this.data.2[,1] <- log(ols.out$residuals^2)
    
    ols.out.2 <- lm(y ~ X1 +X2 ,data=  this.data.2)
    g.hat <- ols.out.2$fitted.values
    h.hat <- exp(g.hat)

    this.data <- cbind(this.data,1)
    colnames(this.data)[ncol(this.data)] <- "X0"

    root.h <- matrix(rep(h.hat,ncol(this.data)),ncol=ncol(this.data))    
    root.h <- sqrt(root.h)
    
    this.data <- this.data/root.h

    lm.out <- lm(y ~ 0 + X0 + X1 +X2 ,data=  this.data)

    fgls.coef[i,1:3,2] <- coef(lm.out)
    fgls.coef[i,4,2] <- summary(lm.out)$sigma
    fgls.coef.se[i,c(1:3),2] <- coef(summary(lm.out))[,2]
    fgls.h.hat[i,,2] <- h.hat
    
    }
dimnames(fgls.coef)[[2]] <- c("b0","b1","b2","sigma")
dimnames(fgls.coef.se)[[2]] <- c("b0","b1","b2")


```






Below, you can see the distribution of the point estimates and standard errors:


```{r}

par(mfrow=c(1,2))
for (i.fig in 1:2){

  target <- paste0("b",i.fig)

    x.range <- range(c(fgls.coef[,target,]))
    plot(density.out <- density(all.coef[,target,1]),
         main=target,xlab=paste0("Estimates"),
         xlim=x.range)
    abline(v=mean(fgls.coef[,target,1]))
    par(new=T)
    plot(density(fgls.coef[,target,2]),ann=F,xlab="",ylab="",main="",
         axes=F,
         xlim=x.range,lty=2,
         ylim=c(0,max(density.out$y)))
    
    sds <- c(sqrt(naive.var(fgls.coef[,target,1])),
             sqrt(naive.var(fgls.coef[,target,2])))
    abline(v=mean(fgls.coef[,target,2]),lty=2)
    legend("topright",lty=c(1,NA,2,NA),
           c("Under GM",
           paste("SD:",round(sds[1],3)),
           "Heteroskedasticity",
           paste("SD:",round(sds[2],3))),bty="n")
  
}
    


par(mfrow=c(1,2))
for (i.fig in 1:2){

  target <- paste0("b",i.fig)

    x.range <- range(c(fgls.coef.se[,target,]))
    plot(density.out <- density(fgls.coef.se[,target,1]),
         main=target,xlab=paste0("Standard errors"),
         xlim=x.range)
    #abline(v=mean(all.coef.se[,target,1]))
    par(new=T)
    plot(density(fgls.coef.se[,target,2]),ann=F,xlab="",ylab="",main="",
         axes=F,
         xlim=x.range,lty=2,
         ylim=c(0,max(density.out$y)))
    means <- c(mean(fgls.coef.se[,target,1]),
               mean(fgls.coef.se[,target,2]))
    #abline(v=mean(all.coef[,target,2]),lty=2)
    legend("topright",lty=c(1,NA,2,NA),
           c("Under GM",
           paste("Mean:",round(means[1],3)),
           "Heteroskedasticity",
           paste("Mean:",round(means[2],3))),bty="n")
    
}

```

It is obvious the FGLS provides smaller variances of their estimates than WLS and OLS. That is, OLS is not the *best* estimator.





```{r}

this.data <- samples.srm.het$generated.data[[1]]
dgp.weight <- this.data$het.weight

par(mfrow=c(1,3))
hist(fgls.h.hat[,,1],main="GM: FGLS",xlab="Estimated h",freq=F)

wls.weight <- samples.srm.het$generated.data[[1]]$X2
plot(dgp.weight,wls.weight,main="Het: WLS",
     xlab="True variance weight",ylab="X2")


est.weight <- fgls.h.hat[,,2]
plot(dgp.weight,est.weight[1,],main="Het: FGLS",
     xlab="True variance weight",ylab="Estimated h")


```





```{r}
knitr::knit_exit()
```






## Linear probability model

```{r}
n.iv <- 1
true.slope <- 0.4
true.intercept <- 0.4

x.mu <- 0.6
x.Sigma <- as.matrix(0.5)

true.err.var <- 0.05

```




We generate `r num.datasets` datasets with n=`r sample.size` under the GM-assumptions. The number of independent variables is `r n.iv`. The true regression line has the intercept of `r true.intercept` and the slope of `r true.slope`. The independent variables are generated with the mean `r x.mu`, variances  `r diag(x.Sigma)`.

In previous examples, we generated the dependent variable by adding random errors to the predicted values generated above. Here, instead, we use the predicted values as probability that the dependent variable has the value 1. With the opposite probability, the dependent variable has the value 0. This is called Bernoulli trial.


```{r}

samples.1 <- data.generation(sample.size=sample.size,
                             n.sim=num.datasets,
                             n.iv=n.iv,
                             x.mu=x.mu,
                             x.Sigma=x.Sigma,
                             para=c(true.intercept,true.slope),
                             err.dist = "normal",
                             err.disp = true.err.var,
                             binary.y = TRUE)


```



```{r}

this.data <- samples.1$generated.data[[1]]

error <- this.data$y - this.data$y.hat.raw



cond.mean.plot(this.data$X1, error ,num.means = 20,
               ylab="Error",xlab="X1")


cond.mean.plot(this.data$X1, error ,num.means = 20,
               ylab="Error",xlab="X1",cond.var = TRUE)



```






## Weighted least squares


```{r}

```






Analogously, we generate further two sets of samples. The second set is generated under the same parameters except that the mean value of X is set to `r x.mu+4` instead of `r x.mu`. The third set is generated under the same parameters of the first set except that the true slope is `r true.slope+0.4` instead of `r true.slope`. 


```{r}

samples.2 <- data.generation(sample.size=sample.size,
                             n.sim=num.datasets,
                             n.iv=n.iv,
                             x.mu=x.mu+4,
                             x.Sigma=x.Sigma,
                             para=c(true.intercept,true.slope),
                             err.dist = "normal",
                             err.disp = true.err.var,
                             binary.y = TRUE)

samples.3 <- data.generation(sample.size=sample.size,
                             n.sim=num.datasets,
                             n.iv=n.iv,
                             x.mu=x.mu,
                             x.Sigma=x.Sigma,
                             para=c(true.intercept,true.slope+0.4),
                             err.dist = "normal",
                             err.disp = true.err.var,
                             binary.y = TRUE)

```


We can estimate the regression model $$\hat y = \tilde \beta_0 + \tilde \beta_1 x_1$$.


```{r}

all.coef <- array(NA,dim=c(num.datasets,3,3))
all.coef.se <- array(NA,dim=c(num.datasets,2,3))
all.predict.outside <- array(NA,dim=c(num.datasets,3,2))

for (i.set in 1:3 ){
  if (i.set ==1 ) this.generated.data <- samples.1
  if (i.set ==2 ) this.generated.data <- samples.2
  if (i.set ==3 ) this.generated.data <- samples.3
for (i in 1:num.datasets){
    this.data <- this.generated.data$generated.data[[i]]

    lm.out <- lm(y ~ X1  ,data=  this.data)

    all.coef[i,1:2,i.set] <- coef(lm.out)
    all.coef[i,3,i.set] <- summary(lm.out)$sigma
    all.coef.se[i,c(1:2),i.set] <- coef(summary(lm.out))[,2]

    all.predict.outside[i,i.set,1] <- sum(this.data$y.hat==0|this.data$y.hat==1)
    all.predict.outside[i,i.set,2] <- sum(lm.out$fitted.values<0|lm.out$fitted.values>1)

    # visual presentation
    if (i ==1){
      plot(this.data$y ~ this.data$X1,
           xlab="X1",ylab="Y",main="")
      abline(coef= this.generated.data$para)
      
      abline(reg=lm.out,lty=2)
      legend("bottomright",lty=c(1,2),
             c("True","Estimated"),
             bty="n")
    }
    
}
}  
dimnames(all.coef)[[2]] <- c("b0","b1","sigma")
dimnames(all.coef.se)[[2]] <- c("b0","b1")

```


Below you will find the distribution of estimated regression coefficients:


```{r}

    x.range <- range(c(all.coef[,"b1",]))
    x.range[2] <- 0.52
    density.out <- density(all.coef[,"b1",1])
    plot(density.out,
         main="",xlab=paste0("Regression coefficients for X1"),
         xlim=x.range,ylim=c(0,max(density.out$y)*1.5))
    abline(v=mean(all.coef[,"b1",1]))
    par(new=T)
    plot(density(all.coef[,"b1",2]),ann=F,xlab="",ylab="",main="",
         axes=F,
         xlim=x.range,lty=2,
         ylim=c(0,max(density.out$y)*1.5))
    abline(v=mean(all.coef[,"b1",2]),lty=2)
    par(new=T)
    plot(density(all.coef[,"b1",3]),ann=F,xlab="",ylab="",main="",
         axes=F,
         xlim=x.range,lty=2,
         ylim=c(0,max(density.out$y)*1.5))
    abline(v=mean(all.coef[,"b1",3]),lty=3)
    legend("topright",lty=c(1:3),c("1st set","2nd set","3rd set"),bty="n")

```

For the first and second set, the true regression slope is `r true.slope` while it is set to `r true.slope+0.4` for the third set. Obviously, the estimates based on the second and third set of samples are downwards biased, while those based on the first set are almost bias free.   

These results seem to have to do with how often the predicted values are outside of the range [0,1]. 


```{r}

par(mfcol=c(2,3))

for (i.fig in 1:3){
  hist(all.predict.outside[,i.fig,1]/sample.size,br=seq(-0.025,1.025,by=0.05),
       xlab="% of outside of [0,1]",
       main=paste(c("1st","2nd","3rd")[i.fig],"set (true)"))

  hist(all.predict.outside[,i.fig,2]/sample.size,br=seq(-0.025,1.025,by=0.05),
       xlab="% of outside of [0,1]",
       main=paste(c("1st","2nd","3rd")[i.fig],"set (estimated)"))

  }






```

The above figure shows in the upper panels how often the predicted values are outside of [0,1] in the data generation process. The lower panels presents, in contrast, how often the predicted values based on the estimated regression model are outside of the range.


These figures are however not crucial for the above bias and estimation error. More important is the difference of the share of predicted values outside of the range between the true model and estimated results. These differences are plotted against the estimation error (estimated slope minus the true slope) in the figure below.



```{r}

par(mfrow=c(1,3))
for (i.fig in 1:3){
  if (i.fig <= 2){
      this.error <- all.coef[,2,i.fig] - true.slope
  } else {
      this.error <- all.coef[,2,i.fig] - true.slope-0.4
  }

  this.share <- (all.predict.outside[,i.fig,2]-all.predict.outside[,i.fig,1])/sample.size
  
  plot(this.share,this.error,
       xlab="%pt diff of outside of [0,1]",
       ylab="Estimation error",
       main=paste(c("1st","2nd","3rd")[i.fig],"set"))
  abline(lm(this.error ~ this.share))
}



```

The first two panels demonstrate that zero difference in the share of predicted values outside of the range is associated with the unbiased results. 




